# Semantic Entity Clustering - Architectural Design

**Owner:** Ada (Orchestration Architect)
**Status:** DESIGN - Ready for Implementation
**Phase:** 3 (blocked by Phase 1 completion)
**Created:** 2025-10-24

---

## Purpose

Discover topic-based semantic entities from node embeddings to complement config-driven functional entities.

**Outcome:** Dual entity types working together:
- **Functional entities** (8 fixed roles: translator, architect, validator...) - config-defined
- **Semantic entities** (dynamic topics discovered from content) - clustering-derived

**Why Both:**
- Functional: Captures MODE of thinking (how I approach work)
- Semantic: Captures TOPIC of thinking (what I'm thinking about)
- Together: Entity-first WM can select "The Architect thinking about database schemas" vs "The Validator thinking about test coverage"

---

## Design Decisions

### 1. Input Data

**Source:** Node embeddings generated by embedding_service (Phase 3 Item 6)

**Eligible node types:**
- Concept, Realization, Personal_Pattern - personal knowledge
- Document, Principle, Mechanism - shared knowledge
- Best_Practice, Anti_Pattern - learned patterns

**Filtering:**
```python
nodes_for_clustering = [
    n for n in graph.nodes.values()
    if n.embedding is not None  # Only embedded nodes
    and n.node_type in CONTENT_BEARING_TYPES
    and n.scope in ["personal", "organizational"]  # Not external ecosystem data
]
```

**Expected scale:**
- Personal graphs: 80-150 embedded nodes per citizen
- Organizational graph: 200-500 embedded nodes
- Per-citizen clustering: cluster citizen's personal nodes only
- Collective clustering: cluster organizational nodes (future Phase 4)

---

### 2. Clustering Algorithm

**Primary choice: HDBSCAN** (Hierarchical Density-Based Spatial Clustering)

**Why HDBSCAN:**
- Discovers natural cluster count (no need to specify K)
- Handles varying cluster densities (some topics dense, others sparse)
- Identifies outliers/noise (nodes that don't belong to any topic)
- Hierarchical structure (can merge/split clusters as graph evolves)
- Production-proven for semantic clustering (used by Zep, LangChain)

**Alternative: Adaptive K-Means**
- Use elbow method + silhouette scores to find optimal K
- Simpler implementation, faster for small graphs
- Fallback if HDBSCAN produces too many/too few clusters

**Parameters:**
```python
clusterer = hdbscan.HDBSCAN(
    min_cluster_size=5,      # Minimum 5 nodes to form a topic
    min_samples=3,            # Minimum 3 similar nodes for core samples
    metric='cosine',          # Cosine similarity for embeddings
    cluster_selection_epsilon=0.1,  # Merge clusters closer than this
    cluster_selection_method='eom'  # Excess of Mass (stable clusters)
)
```

---

### 3. Cluster Count Constraints

**Target range:** 3-8 semantic entities per citizen

**Why this range:**
- Combined with 8 functional entities = 11-16 total entities
- WM selects 5-7 entities per frame → enough diversity without overload
- Too few (<3): Overly broad topics, loses specificity
- Too many (>8): Fragmented topics, cognitive overload

**Enforcement strategy:**

```python
def cluster_with_adaptive_parameters(embeddings, nodes):
    """
    Try HDBSCAN with default params, adjust if outside target range.
    """
    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, metric='cosine')
    labels = clusterer.fit_predict(embeddings)

    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise

    if num_clusters < 3:
        # Too few clusters - decrease min_cluster_size to allow smaller topics
        clusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=2, metric='cosine')
        labels = clusterer.fit_predict(embeddings)
        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    elif num_clusters > 8:
        # Too many clusters - increase min_cluster_size to merge small topics
        clusterer = hdbscan.HDBSCAN(min_cluster_size=8, min_samples=4, metric='cosine')
        labels = clusterer.fit_predict(embeddings)
        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)

    return labels, clusterer
```

**Handling outliers:**
- Nodes labeled `-1` (noise) → don't create entities for these
- OR: Assign low-weight membership to nearest cluster centroid
- Decision: Start with exclusion (cleaner), add soft assignment later if needed

---

### 4. Entity Creation from Clusters

**For each cluster discovered:**

```python
def create_semantic_entity(cluster_id, member_nodes, embeddings):
    """
    Create semantic entity from cluster.

    Returns: Entity node with:
    - Generated name (from topic keywords)
    - Centroid embedding (cluster center in embedding space)
    - Stable color (from centroid position)
    - BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF") memberships (soft assignment by distance)
    """
    # Compute centroid
    cluster_embeddings = [embeddings[i] for i, label in enumerate(labels) if label == cluster_id]
    centroid = np.mean(cluster_embeddings, axis=0)
    centroid = centroid / np.linalg.norm(centroid)  # Normalize to unit length

    # Generate name from most representative nodes
    representative_nodes = find_closest_to_centroid(member_nodes, embeddings, centroid, top_k=3)
    keywords = extract_keywords_from_nodes(representative_nodes)
    entity_name = generate_entity_name(keywords)  # e.g., "consciousness_substrate_architecture"

    # Generate stable color from centroid position
    color = centroid_to_oklch(centroid)

    # Create entity
    entity = Entity(
        name=entity_name,
        kind="semantic",  # Distinguish from functional entities
        description=f"Semantic cluster representing {keywords[:3]} topics",
        centroid_embedding=centroid.tolist(),
        color=color,
        threshold_runtime=0.5,  # Will be adjusted by cohort statistics
        members={}  # Will be populated by membership assignment
    )

    return entity
```

**Name generation strategy:**

```python
def generate_entity_name(keywords):
    """
    Generate semantic entity name from topic keywords.

    Examples:
    - ["consciousness", "substrate", "graph"] → "consciousness_substrate"
    - ["testing", "validation", "verification"] → "testing_validation"
    - ["architecture", "design", "patterns"] → "architecture_design"
    """
    # Take top 2-3 most distinctive keywords
    top_keywords = keywords[:min(3, len(keywords))]

    # Join with underscores, truncate to reasonable length
    name = "_".join(top_keywords)[:40]

    return f"semantic_{name}"
```

---

### 5. Membership Assignment

**Soft assignment:** Nodes can belong to multiple semantic entities with varying weights.

**Distance-based membership:**

```python
def assign_memberships(node, embeddings, semantic_entities):
    """
    Assign BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF") memberships based on distance to entity centroids.

    Strategy:
    - Compute cosine similarity to each entity centroid
    - Convert to membership weight via sigmoid squashing
    - Normalize so total membership across ALL entities ≤ 1
    """
    node_embedding = embeddings[node.id]

    memberships = {}
    for entity in semantic_entities:
        # Cosine similarity (1 = identical, 0 = orthogonal, -1 = opposite)
        similarity = np.dot(node_embedding, entity.centroid_embedding)

        # Convert to membership weight (0-1 range)
        # Only assign membership if similarity > threshold (0.5 = moderate similarity)
        if similarity > 0.5:
            weight = (similarity - 0.5) / 0.5  # Rescale [0.5, 1.0] → [0, 1]
            memberships[entity.id] = weight

    return memberships
```

**Normalization:** Ensure Σ_e m_{i,e} ≤ 1 across BOTH functional + semantic entities

```python
def normalize_all_memberships(graph):
    """
    Normalize memberships so total across ALL entities doesn't exceed 1.

    Handles both functional (config) and semantic (clustering) entities.
    """
    for node in graph.nodes.values():
        total_membership = sum(
            link.weight
            for link in graph.links.values()
            if link.source == node.id and link.link_type == "BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF")"
        )

        if total_membership > 1.0:
            # Rescale all memberships proportionally
            for link in graph.links.values():
                if link.source == node.id and link.link_type == "BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF")":
                    link.weight = link.weight / total_membership
```

---

### 6. Color Generation

**Goal:** Stable, visually distinct colors for semantic entities

**Strategy:** Map centroid position in embedding space → OKLCH color space

```python
def centroid_to_oklch(centroid):
    """
    Convert centroid embedding to stable OKLCH color.

    Uses first 3 dimensions of centroid to control:
    - Dimension 0 → Hue (0-360 degrees)
    - Dimension 1 → Chroma (saturation, 0.1-0.15 for pastel)
    - Dimension 2 → Lightness (0.6-0.8 for readability)

    Returns: "oklch(0.7 0.12 240)" format string
    """
    # Normalize centroid dimensions to [0, 1] range
    d0 = (centroid[0] + 1) / 2  # Cosine similarity range [-1, 1] → [0, 1]
    d1 = (centroid[1] + 1) / 2
    d2 = (centroid[2] + 1) / 2

    # Map to OKLCH
    hue = d0 * 360  # Full color wheel
    chroma = 0.1 + (d1 * 0.05)  # Pastel saturation (0.1-0.15)
    lightness = 0.6 + (d2 * 0.2)  # Readable lightness (0.6-0.8)

    return f"oklch({lightness:.2f} {chroma:.2f} {hue:.0f})"
```

**Why OKLCH:**
- Perceptually uniform (equal distances = equal perceptual difference)
- Predictable lightness control (ensures text contrast)
- Modern CSS support (native in browsers)
- More intuitive than HSL for procedural generation

**Collision handling:**
- If two entity colors too similar (ΔE < threshold), adjust hue by ±30°
- Track color assignments to avoid collisions

---

### 7. Incremental Clustering

**Initial run:** Cluster all embedded nodes at once

**Subsequent runs:**
- Only re-cluster if graph has changed significantly
- Track last_clustered timestamp
- Re-cluster trigger conditions:
  - New nodes embedded > threshold (e.g., 20% growth)
  - Entity energy distributions drift (topics shifting)
  - Manual re-cluster request

**Stability considerations:**
- Cluster assignments should be relatively stable across runs
- Use cluster centroids as seeds for next clustering (warm start)
- Track entity evolution over time (which topics emerge/fade)

```python
def should_recluster(graph, last_clustered_time):
    """
    Determine if semantic entities need re-clustering.
    """
    # Check 1: Significant growth in embedded nodes
    embedded_nodes = [n for n in graph.nodes.values() if n.embedding is not None]
    nodes_since_last = [n for n in embedded_nodes if n.created_at > last_clustered_time]

    growth_rate = len(nodes_since_last) / len(embedded_nodes)
    if growth_rate > 0.2:  # 20% growth
        return True

    # Check 2: Time-based (weekly re-cluster for active graphs)
    if datetime.now() - last_clustered_time > timedelta(days=7):
        return True

    return False
```

---

## Implementation Interface

### Service Class

```python
class SemanticClusteringService:
    """
    Discover semantic entities from node embeddings.

    Usage:
        service = SemanticClusteringService(
            algorithm="hdbscan",
            target_clusters=(3, 8),
            min_cluster_size=5
        )

        entities = await service.cluster_and_create_entities(graph)
        # Returns list of semantic Entity objects
    """

    def __init__(
        self,
        algorithm: str = "hdbscan",
        target_clusters: Tuple[int, int] = (3, 8),
        min_cluster_size: int = 5,
        color_collision_threshold: float = 20.0  # ΔE in OKLCH
    ):
        self.algorithm = algorithm
        self.target_min, self.target_max = target_clusters
        self.min_cluster_size = min_cluster_size
        self.color_collision_threshold = color_collision_threshold

    async def cluster_and_create_entities(
        self,
        graph: Graph
    ) -> List[Entity]:
        """
        Main entry point: cluster embeddings → create semantic entities.

        Steps:
        1. Filter eligible nodes (have embeddings, content-bearing types)
        2. Extract embeddings into matrix
        3. Run clustering algorithm (HDBSCAN or adaptive K-Means)
        4. Create Entity object for each cluster
        5. Assign BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF") memberships
        6. Normalize memberships across all entities

        Returns: List of semantic entities (ready for upsert to graph)
        """
        # Step 1: Filter nodes
        eligible_nodes = self._filter_eligible_nodes(graph)

        if len(eligible_nodes) < self.min_cluster_size * 2:
            logger.warning(f"Not enough nodes for clustering ({len(eligible_nodes)} < {self.min_cluster_size * 2})")
            return []

        # Step 2: Extract embeddings
        embeddings = np.array([n.embedding for n in eligible_nodes])

        # Step 3: Cluster
        labels, clusterer = self._cluster_embeddings(embeddings)

        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        logger.info(f"Discovered {num_clusters} semantic clusters from {len(eligible_nodes)} nodes")

        # Step 4: Create entities
        entities = []
        for cluster_id in set(labels):
            if cluster_id == -1:  # Skip noise
                continue

            cluster_nodes = [n for n, label in zip(eligible_nodes, labels) if label == cluster_id]
            cluster_embeddings = embeddings[labels == cluster_id]

            entity = self._create_entity_from_cluster(
                cluster_id,
                cluster_nodes,
                cluster_embeddings
            )
            entities.append(entity)

        # Step 5: Assign memberships
        for node in eligible_nodes:
            memberships = self._assign_memberships(node, entities)
            for entity_id, weight in memberships.items():
                graph.upsert_belongs_to(node.id, entity_id, weight=weight)

        # Step 6: Normalize (across functional + semantic)
        self._normalize_all_memberships(graph)

        return entities

    def _filter_eligible_nodes(self, graph: Graph) -> List[Node]:
        """Filter nodes eligible for clustering."""
        eligible_types = [
            "Concept", "Realization", "Personal_Pattern",
            "Document", "Principle", "Mechanism",
            "Best_Practice", "Anti_Pattern"
        ]

        return [
            n for n in graph.nodes.values()
            if n.embedding is not None
            and n.node_type in eligible_types
            and n.scope in ["personal", "organizational"]
        ]

    def _cluster_embeddings(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Any]:
        """
        Run clustering algorithm with adaptive parameters.

        Returns: (labels, clusterer)
        """
        if self.algorithm == "hdbscan":
            return self._cluster_hdbscan_adaptive(embeddings)
        elif self.algorithm == "kmeans":
            return self._cluster_kmeans_adaptive(embeddings)
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")

    def _cluster_hdbscan_adaptive(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Any]:
        """
        HDBSCAN with parameter adjustment to hit target cluster count.
        """
        import hdbscan

        # Try default parameters
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=self.min_cluster_size,
            min_samples=max(2, self.min_cluster_size // 2),
            metric='cosine'
        )
        labels = clusterer.fit_predict(embeddings)
        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)

        # Adjust if outside target range
        if num_clusters < self.target_min:
            # Too few - decrease min_cluster_size
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=max(3, self.min_cluster_size - 2),
                min_samples=2,
                metric='cosine'
            )
            labels = clusterer.fit_predict(embeddings)

        elif num_clusters > self.target_max:
            # Too many - increase min_cluster_size
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=self.min_cluster_size + 3,
                min_samples=max(3, (self.min_cluster_size + 3) // 2),
                metric='cosine'
            )
            labels = clusterer.fit_predict(embeddings)

        return labels, clusterer

    def _cluster_kmeans_adaptive(self, embeddings: np.ndarray) -> Tuple[np.ndarray, Any]:
        """
        K-Means with elbow method to find optimal K.
        """
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score

        # Try range of K values
        best_k = self.target_min
        best_score = -1

        for k in range(self.target_min, self.target_max + 1):
            clusterer = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = clusterer.fit_predict(embeddings)
            score = silhouette_score(embeddings, labels, metric='cosine')

            if score > best_score:
                best_score = score
                best_k = k

        # Use best K
        clusterer = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        labels = clusterer.fit_predict(embeddings)

        return labels, clusterer

    def _create_entity_from_cluster(
        self,
        cluster_id: int,
        member_nodes: List[Node],
        cluster_embeddings: np.ndarray
    ) -> Entity:
        """
        Create semantic entity from cluster.
        """
        # Compute centroid
        centroid = np.mean(cluster_embeddings, axis=0)
        centroid = centroid / np.linalg.norm(centroid)

        # Generate name
        keywords = self._extract_keywords(member_nodes)
        name = self._generate_name(keywords)

        # Generate color
        color = self._centroid_to_oklch(centroid)

        # Create entity
        entity = Entity(
            name=name,
            kind="semantic",
            description=f"Semantic cluster: {', '.join(keywords[:3])}",
            centroid_embedding=centroid.tolist(),
            color=color,
            threshold_runtime=0.5,
            members={}
        )

        return entity

    def _extract_keywords(self, nodes: List[Node], top_k: int = 5) -> List[str]:
        """
        Extract representative keywords from cluster nodes.

        Uses TF-IDF to find distinctive terms.
        """
        from sklearn.feature_extraction.text import TfidfVectorizer

        # Combine node names and descriptions
        texts = [f"{n.name} {n.description}" for n in nodes]

        # Extract keywords via TF-IDF
        vectorizer = TfidfVectorizer(max_features=top_k, stop_words='english')
        try:
            tfidf = vectorizer.fit_transform(texts)
            keywords = vectorizer.get_feature_names_out()
            return list(keywords[:top_k])
        except:
            # Fallback: use most common words from node names
            words = " ".join([n.name for n in nodes]).split("_")
            from collections import Counter
            common = Counter(words).most_common(top_k)
            return [word for word, count in common]

    def _generate_name(self, keywords: List[str]) -> str:
        """Generate entity name from keywords."""
        # Take top 2-3 keywords, join with underscores
        top = keywords[:min(3, len(keywords))]
        name = "_".join(top)[:40]  # Truncate to reasonable length
        return f"semantic_{name}"

    def _centroid_to_oklch(self, centroid: np.ndarray) -> str:
        """Convert centroid to OKLCH color."""
        # Use first 3 dimensions
        d0 = (centroid[0] + 1) / 2
        d1 = (centroid[1] + 1) / 2
        d2 = (centroid[2] + 1) / 2

        hue = d0 * 360
        chroma = 0.1 + (d1 * 0.05)
        lightness = 0.6 + (d2 * 0.2)

        return f"oklch({lightness:.2f} {chroma:.2f} {hue:.0f})"

    def _assign_memberships(
        self,
        node: Node,
        entities: List[Entity]
    ) -> Dict[str, float]:
        """Assign memberships based on distance to centroids."""
        if not node.embedding:
            return {}

        node_embedding = np.array(node.embedding)
        memberships = {}

        for entity in entities:
            centroid = np.array(entity.centroid_embedding)

            # Cosine similarity
            similarity = np.dot(node_embedding, centroid)

            # Only assign if similarity > threshold
            if similarity > 0.5:
                weight = (similarity - 0.5) / 0.5  # Rescale [0.5, 1] → [0, 1]
                memberships[entity.id] = weight

        return memberships

    def _normalize_all_memberships(self, graph: Graph):
        """Normalize memberships across all entities."""
        for node in graph.nodes.values():
            total = sum(
                link.weight
                for link in graph.links.values()
                if link.source == node.id and link.link_type == "BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF")"
            )

            if total > 1.0:
                for link in graph.links.values():
                    if link.source == node.id and link.link_type == "BELONGS_TO (Deprecated - now "MEMBER_OF") (Deprecated - now "MEMBER_OF")":
                        link.weight = link.weight / total
```

---

## Integration Points

### 1. Call from Bootstrap (After Embeddings)

```python
# In websocket_server.py after embedding_service completes

if config.get("semantic_clustering_enabled", True):
    logger.info(f"[{citizen_id}] Discovering semantic entities...")

    from orchestration.services.semantic_clustering import SemanticClusteringService
    clustering_svc = SemanticClusteringService(
        algorithm="hdbscan",
        target_clusters=(3, 8)
    )

    semantic_entities = await clustering_svc.cluster_and_create_entities(graph)
    logger.info(f"[{citizen_id}] Created {len(semantic_entities)} semantic entities")

    # Upsert to graph
    for entity in semantic_entities:
        graph.subentities[entity.id] = entity

    # Persist
    adapter.persist_subentities(graph)
```

### 2. Scheduled Re-Clustering

```python
# Run weekly or on-demand for active graphs

async def refresh_semantic_entities(graph_id: str):
    graph = adapter.load_graph(graph_id)

    # Check if re-cluster needed
    if should_recluster(graph, graph.last_clustered_time):
        logger.info(f"[{graph_id}] Re-clustering semantic entities...")

        # Remove old semantic entities
        old_semantic = [e for e in graph.subentities.values() if e.kind == "semantic"]
        for e in old_semantic:
            del graph.subentities[e.id]

        # Discover new semantic entities
        service = SemanticClusteringService()
        new_semantic = await service.cluster_and_create_entities(graph)

        for entity in new_semantic:
            graph.subentities[entity.id] = entity

        graph.last_clustered_time = datetime.now()
        adapter.persist_subentities(graph)

        logger.info(f"[{graph_id}] Re-clustering complete: {len(new_semantic)} entities")
```

---

## Observability

### Metrics

- `clustering.entities_created.total` (counter) - semantic entities created
- `clustering.nodes_clustered.total` (counter) - nodes assigned to clusters
- `clustering.noise_nodes.total` (counter) - nodes excluded (outliers)
- `clustering.duration_seconds` (histogram) - clustering execution time

### Logs

```
INFO: [citizen_ada] Semantic clustering starting (algorithm=hdbscan, nodes=127)
INFO: [citizen_ada] Discovered 5 clusters from 127 nodes (22 noise)
INFO: [citizen_ada] Cluster 0: 34 nodes, keywords=['consciousness', 'substrate', 'graph']
INFO: [citizen_ada] Cluster 1: 28 nodes, keywords=['testing', 'validation', 'verification']
INFO: [citizen_ada] Cluster 2: 22 nodes, keywords=['architecture', 'design', 'patterns']
INFO: [citizen_ada] Cluster 3: 19 nodes, keywords=['orchestration', 'coordination', 'integration']
INFO: [citizen_ada] Cluster 4: 14 nodes, keywords=['documentation', 'specs', 'reference']
INFO: [citizen_ada] Created 5 semantic entities (total: 8.3s)
INFO: [citizen_ada] Persisted entities to FalkorDB
```

---

## Testing Strategy

### Unit Tests

- `test_filter_eligible_nodes()` - correct filtering
- `test_clustering_produces_target_range()` - cluster count in bounds
- `test_membership_normalization()` - total ≤ 1 per node
- `test_centroid_to_color()` - stable color generation
- `test_keyword_extraction()` - meaningful keywords

### Integration Tests

- `test_cluster_small_graph()` - 50 nodes → 3-5 entities
- `test_cluster_large_graph()` - 200 nodes → 6-8 entities
- `test_recluster_stability()` - similar clusters across runs
- `test_combined_functional_semantic()` - both entity types coexist

### Acceptance

- Run on citizen_ada graph (127 embedded nodes)
- Verify 3-8 semantic entities created
- Verify entity names meaningful (keywords match content)
- Verify colors visually distinct
- Verify memberships sum to ≤ 1 per node
- Verify entities persist to FalkorDB
- Verify WM selection includes both functional + semantic entities

---

## Configuration

`orchestration/config/semantic_clustering.yml`:

```yaml
semantic_clustering:
  enabled: true
  algorithm: "hdbscan"  # or "kmeans"
  target_clusters:
    min: 3
    max: 8

  hdbscan:
    min_cluster_size: 5
    min_samples: 3
    metric: "cosine"

  kmeans:
    n_init: 10
    random_state: 42

  membership:
    similarity_threshold: 0.5  # Minimum cosine similarity for membership

  color:
    collision_threshold: 20.0  # ΔE in OKLCH

  reclustering:
    enabled: true
    growth_threshold: 0.2  # Re-cluster if 20% new nodes
    time_interval_days: 7  # Re-cluster weekly
```

---

## Dependencies

**Requires:**
- Embedding service (Phase 3 Item 6) completed
- Node embeddings generated and persisted
- FalkorDB adapter supports Entity storage

**Python packages:**
- `hdbscan` - hierarchical clustering
- `scikit-learn` - K-Means fallback, TF-IDF for keywords
- `numpy` - array operations

**Blocks:**
- Phase 2 (visualization) - needs semantic entities to render
- Full entity-first WM - needs both functional + semantic entities

---

## Acceptance Criteria

**Ready for Implementation When:**
- Phase 3 Item 6 (embeddings) complete
- At least 50 embedded nodes exist in graph
- FalkorDB entity persistence working

**Definition of Done:**
- Service class implemented with all methods
- Unit tests passing (>90% coverage)
- Integration test passing on test graph
- Semantic entities created for citizen_ada graph
- Entities persisted to FalkorDB
- Combined functional + semantic entities working together
- WM selection includes both entity types
- Metrics/logs emitting correctly
- Ready for Phase 2 (visualization)

---

**Architect:** Ada "Bridgekeeper"
**Status:** DESIGN COMPLETE - Ready for Implementation
**Next:** Wait for Phase 1 completion + Phase 3 Item 6 (embeddings), then implement
