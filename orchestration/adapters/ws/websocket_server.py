"""
Mind Protocol WebSocket Server

Serves real-time consciousness operations to the dashboard via WebSocket.

Provides:
- WebSocket endpoint at ws://localhost:8000/api/ws
- REST API for consciousness control (pause/resume/speed)
- Real-time event broadcasting (entity_activity, threshold_crossing, consciousness_state)
- Heartbeat file writing for health monitoring

Usage:
    python orchestration/websocket_server.py

Dashboard Connection:
    const ws = new WebSocket('ws://localhost:8000/api/ws');
    ws.onmessage = (event) => {
        const data = JSON.parse(event.data);
        console.log(data.type, data);
    };

Author: Iris "The Aperture" (WebSocket infrastructure)
Integration: Felix "Ironhand" (consciousness engine)
Date: 2025-10-19
"""

import asyncio
import json
import logging
import os
import sys
import atexit
from datetime import datetime, timezone
from pathlib import Path

# Add parent directory for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

# SINGLETON GUARD - Prevent duplicate instances from spawn loops
def acquire_singleton_lock():
    """
    Acquire PID lock to ensure only one instance runs.
    If another instance is running, exit immediately.
    """
    import psutil

    lock_dir = Path(__file__).parent.parent.parent.parent / ".locks"
    lock_dir.mkdir(parents=True, exist_ok=True)
    lock_file = lock_dir / "websocket_server.pid"

    if lock_file.exists():
        try:
            pid = int(lock_file.read_text().strip())
            if psutil.pid_exists(pid):
                # Another instance is running
                sys.stderr.write(f"[SINGLETON] websocket_server already running (PID {pid}). Exiting.\n")
                sys.exit(0)
            else:
                # Stale lock - remove it
                lock_file.unlink(missing_ok=True)
        except (ValueError, OSError):
            # Corrupt lock - remove it
            lock_file.unlink(missing_ok=True)

    # Write our PID
    lock_file.write_text(str(os.getpid()))

    # Register cleanup on exit
    def cleanup_lock():
        try:
            if lock_file.exists():
                current_pid = int(lock_file.read_text().strip())
                if current_pid == os.getpid():
                    lock_file.unlink(missing_ok=True)
        except:
            pass

    atexit.register(cleanup_lock)

# Acquire lock BEFORE any other imports/initialization
acquire_singleton_lock()

import uvicorn
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import redis
import shutil

from orchestration.adapters.api.control_api import router, websocket_manager
from orchestration.adapters.ws.stream_aggregator import get_stream_aggregator
from orchestration.adapters.api.citizen_snapshot import router as citizen_router
from orchestration.adapters.api.docs_view_api_v2 import router as docs_router
from orchestration.mechanisms.consciousness_engine_v2 import ConsciousnessEngineV2, EngineConfig
from orchestration.adapters.storage.engine_registry import register_engine, CONSCIOUSNESS_TASKS, get_all_engines
from orchestration.libs.utils.falkordb_adapter import FalkorDBAdapter
from orchestration.libs.websocket_broadcast import ConsciousnessStateBroadcaster
from orchestration.core.graph import Graph
from llama_index.graph_stores.falkordb import FalkorDBGraphStore
from orchestration.services.health import StartupSelfTests
from orchestration.services.economy import initialize_economy_runtime
from orchestration.services.topology import TopologyAnalyzerService

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Economy runtime handle
ECONOMY_RUNTIME = None

# Topology analyzer services (one per citizen)
TOPOLOGY_ANALYZERS = {}

# Dashboard State Aggregator (1Hz emission service)
DASHBOARD_AGGREGATOR = None

# Create FastAPI app
app = FastAPI(
    title="Mind Protocol Consciousness API",
    description="WebSocket and REST API for consciousness operations",
    version="2.0.0"
)

# Enable CORS for Next.js dashboard (local + production)
# Read ALLOWED_ORIGINS from environment (comma-separated), default to localhost for dev
allowed_origins_env = os.getenv("ALLOWED_ORIGINS", "")
allowed_origins = [
    "http://localhost:3000",
    "http://localhost:3001",
    "http://localhost:3002",
    "https://www.mindprotocol.ai",  # Production dashboard
    "https://mindprotocol.ai"       # Production dashboard (without www)
]

# Add production origins from environment
if allowed_origins_env:
    production_origins = [origin.strip() for origin in allowed_origins_env.split(",")]
    allowed_origins.extend(production_origins)

logger.info(f"[CORS] Allowed origins: {allowed_origins}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include control API router (adds /api/* endpoints including /api/ws)
app.include_router(router)
app.include_router(citizen_router)  # Citizen snapshot endpoint (stops 404 flood)
app.include_router(docs_router)  # Docs-as-views endpoints (L3)


# ============================================================================
# Browser Console Logging & Screenshot Endpoints (for Claude Code observability)
# ============================================================================

class LogEntry(BaseModel):
    timestamp: int
    type: str
    message: str
    filename: Optional[str] = None
    lineno: Optional[int] = None
    stack: Optional[str] = None

class LogBatch(BaseModel):
    logs: List[LogEntry]

# Ensure directories exist (project root)
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
LOGS_DIR = PROJECT_ROOT / "claude-logs"
SCREENSHOTS_DIR = PROJECT_ROOT / "claude-screenshots"
LOGS_DIR.mkdir(exist_ok=True)
SCREENSHOTS_DIR.mkdir(exist_ok=True)

# DISABLED: REST API endpoints removed per architectural decision (2025-10-30)
# System uses WebSocket-only communication - no REST API
#
# @app.post("/api/logs")
# async def receive_browser_logs(batch: LogBatch):
#     """[DISABLED] Browser log receiver"""
#     pass


# @app.post("/api/screenshot")  # DISABLED: WebSocket-only architecture


# === Control API Endpoints ===
# Stimulus injection now handled by control_api.py router (included at line 124)
# Route: POST /api/engines/{id}/inject with async injection + persistence


# === Consciousness Engine Management ===

# NOTE: consciousness_tasks moved to engine_registry.CONSCIOUSNESS_TASKS
# Import it so engines started here are tracked globally


from orchestration.config.settings import settings
from orchestration.config.graph_names import resolver

def discover_graphs() -> dict:
    """
    Discover all consciousness graphs from FalkorDB.

    Returns:
        {
            "n1": ["mind-protocol_felix", ...],
            "n2": ["mind-protocol", ...],
            "n3": ["ecosystem", ...]
        }
    """
    logger.info("[Discovery] Connecting to FalkorDB to discover graphs...")

    r = redis.Redis(host=settings.FALKORDB_HOST, port=settings.FALKORDB_PORT, decode_responses=True)

    try:
        # Get list of all graphs
        graphs = r.execute_command("GRAPH.LIST")

        # Categorize by network level using new naming convention
        # N1 citizens: mind-protocol_<name> (not org/ecosystem)
        n1_graphs = [g for g in graphs if g.startswith("mind-protocol_") and g != resolver.org_base()]

        # N2 organizations: mind-protocol
        n2_graphs = [g for g in graphs if g == resolver.org_base()]

        # N3 ecosystem: ecosystem
        n3_graphs = [g for g in graphs if g == "ecosystem"]

        logger.info(f"[Discovery] Found {len(n1_graphs)} N1 citizen graphs: {n1_graphs}")
        logger.info(f"[Discovery] Found {len(n2_graphs)} N2 organizational graphs: {n2_graphs}")
        logger.info(f"[Discovery] Found {len(n3_graphs)} N3 ecosystem graphs: {n3_graphs}")

        return {
            "n1": n1_graphs,
            "n2": n2_graphs,
            "n3": n3_graphs
        }

    except Exception as e:
        logger.error(f"[Discovery] Failed to discover graphs: {e}")
        return {"n1": [], "n2": [], "n3": []}


# @app.get("/api/graphs")  # DISABLED: WebSocket-only architecture


def extract_citizen_id(graph_name: str) -> str:
    """
    Extract citizen ID from hierarchical graph name.

    Examples:
        mind-protocol_felix -> mind-protocol_felix
        mind-protocol -> mind-protocol
        ecosystem -> ecosystem

    Returns the FULL hierarchical name as the citizen_id (no prefix stripping).
    """
    # Return full hierarchical name - this is the canonical citizen_id
    return graph_name


def discover_entities(graph_store: FalkorDBGraphStore) -> list:
    """
    Discover subentities from graph metadata.

    Returns:
        List of subentity IDs (e.g., ["builder", "observer"])
    """
    try:
        # Try to find AI_Agent nodes (N2)
        result = graph_store.query("""
            MATCH (n:AI_Agent)
            WHERE n.entity_id IS NOT NULL
            RETURN n.entity_id
            LIMIT 10
        """)

        if result:
            subentities = [row[0] for row in result]
            logger.info(f"[Discovery] Found subentities from AI_Agent nodes: {subentities}")
            return subentities

        # Try to find Subentity nodes (N1)
        result = graph_store.query("""
            MATCH (n:Subentity)
            WHERE n.id IS NOT NULL
            RETURN n.id
            LIMIT 10
        """)

        if result:
            subentities = [row[0] for row in result]
            logger.info(f"[Discovery] Found subentities from Subentity nodes: {subentities}")
            return subentities

        # Fallback to default
        logger.info(f"[Discovery] No subentities found in graph, using defaults")
        return ["builder", "observer"]

    except Exception as e:
        logger.warning(f"[Discovery] Failed to discover subentities: {e}, using defaults")
        return ["builder", "observer"]


async def start_citizen_consciousness(
    graph_name: str,
    citizen_id: str
) -> ConsciousnessEngineV2:
    """
    Start consciousness for one citizen (N1).

    Returns:
        Running ConsciousnessEngineV2 instance
    """
    logger.info(f"[N1:{citizen_id}] Creating consciousness engine V2...")

    # Create FalkorDB graph store and adapter
    graph_store = FalkorDBGraphStore(
        database=graph_name,  # FIXED: use 'database' not 'graph_name'
        url=settings.FALKORDB_URL
    )
    # Set name attribute for WriteGate namespace enforcement
    graph_store.name = graph_name
    adapter = FalkorDBAdapter(graph_store)

    # Run blocking load_graph in thread with 60s timeout
    logger.info(f"[N1:{citizen_id}] Loading graph from FalkorDB (60s timeout)...")
    loop = asyncio.get_event_loop()
    try:
        graph = await asyncio.wait_for(
            loop.run_in_executor(None, adapter.load_graph, graph_name),
            timeout=60.0
        )
        logger.info(f"[N1:{citizen_id}] Graph loaded successfully")
        # DEBUG: Log what was actually loaded
        logger.info(f"[N1:{citizen_id}] DEBUG: Loaded {len(graph.nodes)} nodes, {len(graph.links)} links")
        logger.info(f"[N1:{citizen_id}] DEBUG: graph.subentities type: {type(graph.subentities)}, is None: {graph.subentities is None}")
        if graph.subentities:
            logger.info(f"[N1:{citizen_id}] DEBUG: Loaded {len(graph.subentities)} subentities")
            for eid in list(graph.subentities.keys())[:3]:
                logger.info(f"[N1:{citizen_id}] DEBUG:   - {eid}")
        else:
            logger.warning(f"[N1:{citizen_id}] DEBUG: graph.subentities is EMPTY!")
    except asyncio.TimeoutError:
        logger.error(f"[N1:{citizen_id}] Graph load timed out after 60s")
        raise RuntimeError(f"Graph load timeout for {graph_name}")

    # === EMERGENCE-ONLY ARCHITECTURE (NO BOOTSTRAP) ===
    # Per specs/v2/subentity_layer/subentity_emergence.md:
    # "Emergence detection happens **during stimulus injection**, not on periodic timer."
    # "Consciousness doesn't wake up every hour and ask 'do I have new parts?' Parts emerge
    # **in the moment** when you encounter something that doesn't fit. The stimulus IS the trigger."
    #
    # SubEntities will emerge naturally via gap detection during stimulus injection:
    # 1. Stimulus arrives ‚Üí retrieval finds nodes ‚Üí gap detection runs
    # 2. If gap detected (novelty/tension/coverage) ‚Üí coalition assembly ‚Üí validation
    # 3. Engine validates proposal ‚Üí ACCEPT (spawn) / REDIRECT / REJECT
    # 4. Membership weights learn continuously via co-activation
    #
    # First stimulus when zero SubEntities exist ‚Üí bootstrap scenario (high structural gap) ‚Üí ACCEPT
    #
    # NO scheduled clustering, NO periodic bootstrap
    # ================================================

    current_entity_count = len(graph.subentities) if graph.subentities else 0
    if current_entity_count == 0:
        logger.info(f"[N1:{citizen_id}] Starting with zero SubEntities - will emerge via stimulus-driven gap detection")
        logger.info(f"[N1:{citizen_id}] First stimulus will trigger bootstrap scenario (high structural gap ‚Üí ACCEPT)")
    else:
        logger.info(f"[N1:{citizen_id}] Loaded {current_entity_count} SubEntities from FalkorDB")

    # Metrics: entities.total gauge
    logger.info(f"[N1:{citizen_id}] entities.total={current_entity_count}")
    logger.info(f"[N1:{citizen_id}] DEBUG CHECKPOINT A: graph.subentities has {len(graph.subentities) if graph.subentities else 0} items BEFORE engine creation")

    # Create engine configuration
    config = EngineConfig(
        tick_interval_ms=10000,  # EMERGENCY THROTTLE: 10s tick interval (was 100ms)
        entity_id=citizen_id,
        network_id="N1",
        enable_diffusion=True,
        enable_decay=True,
        enable_strengthening=True,
        enable_websocket=True
    )

    # Create V2 engine
    engine = ConsciousnessEngineV2(graph, adapter, config)

    # DEBUG: Check graph state immediately after engine creation
    logger.info(f"[N1:{citizen_id}] DEBUG CHECKPOINT B: AFTER engine creation, engine.graph.subentities has {len(engine.graph.subentities) if engine.graph.subentities else 0} items")
    logger.info(f"[N1:{citizen_id}] DEBUG: engine.graph object id={id(engine.graph)}, same as loaded graph? {id(engine.graph) == id(graph)}")

    # Note: V2 doesn't have add_sub_entity or enable_dynamic_prompts methods
    # Sub-entities are managed internally by the engine
    # Dynamic prompts would need to be handled separately if needed

    # Register engine with control API (IN THIS PROCESS!)
    register_engine(citizen_id, engine)

    logger.info(f"[N1:{citizen_id}] ‚úÖ Consciousness engine V2 ready")
    logger.info(f"[N1:{citizen_id}]   Graph nodes: {len(graph.nodes)}, links: {len(graph.links)}")

    # Seed streaming working set so clients can reconstruct without snapshots
    try:
        stream_aggregator = get_stream_aggregator()
        await stream_aggregator.seed_from_graph(citizen_id, graph, cause="bootstrap")
        logger.info(f"[N1:{citizen_id}] Stream aggregator seeded ({len(graph.nodes)} nodes, {len(graph.links)} links)")
    except Exception as exc:
        logger.warning(f"[N1:{citizen_id}] Stream aggregator seed failed: {exc}")

    # Populate snapshot cache with initial graph state (for replay-on-connect)
    try:
        from orchestration.adapters.ws.snapshot_cache import get_snapshot_cache
        cache = get_snapshot_cache()

        logger.info(f"[N1:{citizen_id}] Populating snapshot cache ({len(graph.nodes)} nodes, {len(graph.links)} links)")

        # Cache all nodes
        for node in graph.nodes.values():
            cache.upsert_node(citizen_id, {
                "id": node.id,
                "name": node.name,
                "type": getattr(node.node_type, "value", str(node.node_type)),
                "energy": float(getattr(node, "E", 0.0)),
                "theta": float(getattr(node, "theta", 0.0)),
                "properties": dict(getattr(node, "properties", {}) or {})
            })

        # Cache all links
        for link in graph.links.values():
            cache.upsert_link(citizen_id, {
                "id": link.id,
                "source": link.source_id,
                "target": link.target_id,
                "type": getattr(link.link_type, "value", str(link.link_type)),
                "weight": float(getattr(link, "weight", 0.0)),
                "properties": dict(getattr(link, "properties", {}) or {})
            })

        # Cache all subentities
        if hasattr(graph, 'subentities') and graph.subentities:
            for entity in graph.subentities.values():
                cache.upsert_subentity(citizen_id, {
                    "id": entity.id,
                    "name": getattr(entity, "role_or_topic", entity.id),
                    "kind": getattr(entity, "entity_kind", "functional"),
                    "energy": float(getattr(entity, "energy_runtime", 0.0)),
                    "threshold": float(getattr(entity, "threshold_runtime", 0.0)),
                    "activation_level": getattr(entity, "activation_level_runtime", "absent"),
                    "member_count": int(getattr(entity, "member_count", 0)),
                    "quality": float(getattr(entity, "quality_score", 0.0)),
                    "stability": getattr(entity, "stability_state", "candidate")
                })

        logger.info(f"[N1:{citizen_id}] ‚úÖ Snapshot cache populated (will replay on client connect)")
    except Exception as exc:
        logger.warning(f"[N1:{citizen_id}] Failed to populate snapshot cache: {exc}")

    return engine


async def start_organizational_consciousness(
    graph_name: str,
    org_id: str
) -> ConsciousnessEngineV2:
    """
    Start consciousness for organization (N2).

    Returns:
        Running ConsciousnessEngineV2 instance
    """
    logger.info(f"[N2:{org_id}] Creating organizational consciousness engine V2...")

    # Create FalkorDB graph store and adapter
    graph_store = FalkorDBGraphStore(
        database=graph_name,  # FIXED: use 'database' not 'graph_name'
        url=settings.FALKORDB_URL
    )
    # Set name attribute for WriteGate namespace enforcement
    graph_store.name = graph_name
    adapter = FalkorDBAdapter(graph_store)

    # Run blocking load_graph in thread with 60s timeout
    logger.info(f"[N2:{org_id}] Loading graph from FalkorDB (60s timeout)...")
    loop = asyncio.get_event_loop()
    try:
        graph = await asyncio.wait_for(
            loop.run_in_executor(None, adapter.load_graph, graph_name),
            timeout=60.0
        )
        logger.info(f"[N2:{org_id}] Graph loaded successfully")
    except asyncio.TimeoutError:
        logger.error(f"[N2:{org_id}] Graph load timed out after 60s")
        raise RuntimeError(f"Graph load timeout for {graph_name}")

    # === EMERGENCE-ONLY ARCHITECTURE (NO BOOTSTRAP) ===
    # Per specs/v2/subentity_layer/subentity_emergence.md:
    # SubEntities emerge during stimulus injection via gap detection, not via scheduled clustering.
    # First stimulus when zero SubEntities exist ‚Üí bootstrap scenario (high structural gap) ‚Üí ACCEPT
    # ================================================

    current_entity_count = len(graph.subentities) if graph.subentities else 0
    if current_entity_count == 0:
        logger.info(f"[N2:{org_id}] Starting with zero SubEntities - will emerge via stimulus-driven gap detection")
    else:
        logger.info(f"[N2:{org_id}] Loaded {current_entity_count} SubEntities from FalkorDB")

    # Metrics: entities.total gauge
    logger.info(f"[N2:{org_id}] entities.total={current_entity_count}")

    # Create engine configuration
    config = EngineConfig(
        tick_interval_ms=10000,  # EMERGENCY THROTTLE: 10s tick interval (was 100ms)
        entity_id=org_id,
        network_id="N2",
        enable_diffusion=True,
        enable_decay=True,
        enable_strengthening=True,
        enable_websocket=True
    )

    # Create V2 engine
    engine = ConsciousnessEngineV2(graph, adapter, config)

    # Note: V2 doesn't have add_sub_entity or enable_dynamic_prompts methods
    # Sub-entities are managed internally by the engine
    # Dynamic prompts would need to be handled separately if needed

    # Register engine with control API (IN THIS PROCESS!)
    register_engine(org_id, engine)

    logger.info(f"[N2:{org_id}] ‚úÖ Organizational consciousness engine V2 ready")
    logger.info(f"[N2:{org_id}]   Graph nodes: {len(graph.nodes)}, links: {len(graph.links)}")

    try:
        stream_aggregator = get_stream_aggregator()
        await stream_aggregator.seed_from_graph(org_id, graph, cause="bootstrap")
        logger.info(f"[N2:{org_id}] Stream aggregator seeded ({len(graph.nodes)} nodes, {len(graph.links)} links)")
    except Exception as exc:
        logger.warning(f"[N2:{org_id}] Stream aggregator seed failed: {exc}")

    # Populate snapshot cache with initial graph state (for replay-on-connect)
    try:
        from orchestration.adapters.ws.snapshot_cache import get_snapshot_cache
        cache = get_snapshot_cache()

        logger.info(f"[N2:{org_id}] Populating snapshot cache ({len(graph.nodes)} nodes, {len(graph.links)} links)")

        # Cache all nodes
        for node in graph.nodes.values():
            cache.upsert_node(org_id, {
                "id": node.id,
                "name": node.name,
                "type": getattr(node.node_type, "value", str(node.node_type)),
                "energy": float(getattr(node, "E", 0.0)),
                "theta": float(getattr(node, "theta", 0.0)),
                "properties": dict(getattr(node, "properties", {}) or {})
            })

        # Cache all links
        for link in graph.links.values():
            cache.upsert_link(org_id, {
                "id": link.id,
                "source": link.source_id,
                "target": link.target_id,
                "type": getattr(link.link_type, "value", str(link.link_type)),
                "weight": float(getattr(link, "weight", 0.0)),
                "properties": dict(getattr(link, "properties", {}) or {})
            })

        # Cache all subentities
        if hasattr(graph, 'subentities') and graph.subentities:
            for entity in graph.subentities.values():
                cache.upsert_subentity(org_id, {
                    "id": entity.id,
                    "name": getattr(entity, "role_or_topic", entity.id),
                    "kind": getattr(entity, "entity_kind", "functional"),
                    "energy": float(getattr(entity, "energy_runtime", 0.0)),
                    "threshold": float(getattr(entity, "threshold_runtime", 0.0)),
                    "activation_level": getattr(entity, "activation_level_runtime", "absent"),
                    "member_count": int(getattr(entity, "member_count", 0)),
                    "quality": float(getattr(entity, "quality_score", 0.0)),
                    "stability": getattr(entity, "stability_state", "candidate")
                })

        logger.info(f"[N2:{org_id}] ‚úÖ Snapshot cache populated (will replay on client connect)")
    except Exception as exc:
        logger.warning(f"[N2:{org_id}] Failed to populate snapshot cache: {exc}")

    return engine


# === Heartbeat Writer ===

class HeartbeatWriter:
    """Writes heartbeat file to signal this server is alive."""

    def __init__(self):
        self.heartbeat_dir = Path(__file__).parent.parent / ".heartbeats"
        self.heartbeat_path = self.heartbeat_dir / "websocket_server.heartbeat"
        self.status_file_path = Path(__file__).parent.parent / ".guardian_status.json"
        self.running = False
        self.task = None
        self.start_time = datetime.now(timezone.utc)

        # Create heartbeats directory if needed
        self.heartbeat_dir.mkdir(exist_ok=True)

    async def write_heartbeat(self):
        """Write a single heartbeat file."""
        try:
            heartbeat_data = {
                "component": "websocket_server",
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "pid": os.getpid(),
                "status": "active",
                "connected_clients": len(websocket_manager.active_connections)
            }

            # Atomic write (temp file + rename)
            temp_path = self.heartbeat_path.with_suffix('.tmp')
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(heartbeat_data, f, indent=2)
            temp_path.replace(self.heartbeat_path)

            # Write consolidated consciousness_engine heartbeat
            from orchestration.adapters.storage.engine_registry import CONSCIOUSNESS_ENGINES

            if CONSCIOUSNESS_ENGINES:
                engine_heartbeat = {
                    "component": "consciousness_engine",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "pid": os.getpid(),
                    "status": "active",
                    "engine_count": len(CONSCIOUSNESS_ENGINES),
                    "engines": {
                        citizen_id: {
                            "tick_count": engine.tick_count,
                            "running": engine.running,
                            "nodes": len(engine.graph.nodes),
                            "links": len(engine.graph.links),
                            "subentities": len(engine.graph.subentities) if hasattr(engine.graph, 'subentities') and engine.graph.subentities else 0
                        }
                        for citizen_id, engine in CONSCIOUSNESS_ENGINES.items()
                    }
                }

                engine_heartbeat_path = self.heartbeat_dir / "consciousness_engine.heartbeat"
                temp_path = engine_heartbeat_path.with_suffix('.tmp')
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(engine_heartbeat, f, indent=2)
                temp_path.replace(engine_heartbeat_path)

        except Exception as e:
            logger.error(f"Failed to write heartbeat: {e}")

    def check_service_heartbeat(self, service_name, max_age_seconds=30):
        """Check if a service heartbeat is fresh."""
        heartbeat_file = self.heartbeat_dir / f"{service_name}.heartbeat"

        try:
            if not heartbeat_file.exists():
                return {"status": "unknown", "reason": "no_heartbeat_file"}

            with open(heartbeat_file, 'r') as f:
                data = json.load(f)

            timestamp_str = data.get("timestamp")
            if not timestamp_str:
                return {"status": "unknown", "reason": "no_timestamp"}

            heartbeat_time = datetime.fromisoformat(timestamp_str)
            now = datetime.now(timezone.utc)
            age_seconds = (now - heartbeat_time).total_seconds()

            if age_seconds > max_age_seconds:
                return {
                    "status": "stale",
                    "age_seconds": age_seconds,
                    "pid": data.get("pid"),
                    "last_seen": timestamp_str
                }
            else:
                return {
                    "status": "healthy",
                    "age_seconds": age_seconds,
                    "pid": data.get("pid")
                }

        except Exception as e:
            return {"status": "error", "reason": str(e)}

    def check_dashboard_health(self):
        """Check if Next.js dashboard is accessible on port 3000.

        Uses 15s timeout to account for Next.js compilation time during
        startup or hot reload. This prevents false negatives when dashboard
        is healthy but still compiling routes.
        """
        import socket
        import urllib.request

        try:
            # First check if port 3000 is bound
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(2)
            result = sock.connect_ex(('localhost', 3000))
            sock.close()

            if result != 0:
                return {
                    "status": "down",
                    "reason": "Port 3000 not bound"
                }

            # Port is bound, now check HTTP responsiveness
            # 15s timeout accounts for Next.js compilation during startup/hot-reload
            try:
                req = urllib.request.Request('http://localhost:3000', method='HEAD')
                with urllib.request.urlopen(req, timeout=15) as response:
                    if response.status == 200:
                        return {"status": "healthy"}
                    else:
                        return {
                            "status": "degraded",
                            "reason": f"HTTP {response.status}"
                        }
            except urllib.error.URLError:
                return {
                    "status": "degraded",
                    "reason": "HTTP request failed"
                }

        except Exception as e:
            return {
                "status": "error",
                "reason": str(e)
            }

    async def write_guardian_status(self):
        """Write guardian status to root CLAUDE.md for automatic injection."""
        try:
            claude_md_path = Path(__file__).parent.parent / "CLAUDE.md"

            # Check all service heartbeats
            conversation_watcher_status = self.check_service_heartbeat("conversation_watcher")
            consciousness_engine_status = self.check_service_heartbeat("consciousness_engine")
            dashboard_status = self.check_dashboard_health()
            websocket_status = {
                "status": "healthy",
                "pid": os.getpid(),
                "port": 8000,
                "connected_clients": len(websocket_manager.active_connections)
            }

            # Calculate uptime
            uptime_seconds = (datetime.now(timezone.utc) - self.start_time).total_seconds()

            # Determine overall status
            all_healthy = (
                conversation_watcher_status["status"] == "healthy" and
                consciousness_engine_status["status"] == "healthy" and
                dashboard_status["status"] == "healthy" and
                websocket_status["status"] == "healthy"
            )

            overall_status = "operational" if all_healthy else "degraded"

            # Build service status dict for message generation
            service_statuses = {
                'memory': {
                    'status': conversation_watcher_status["status"],
                    'issue': conversation_watcher_status.get("reason", "Service heartbeat stale")
                },
                'graph': {
                    'status': consciousness_engine_status["status"],
                    'issue': consciousness_engine_status.get("reason", "Engine heartbeat stale")
                },
                'dashboard': {
                    'status': dashboard_status["status"],
                    'issue': dashboard_status.get("reason", "Dashboard unreachable")
                }
            }

            # Generate status message
            status_message = self._generate_status_message(overall_status, uptime_seconds, service_statuses)

            # Write pure status content (no delimiters, no wrapper)
            new_content = status_message

            # Atomic write
            temp_path = claude_md_path.with_suffix('.tmp')
            with open(temp_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            temp_path.replace(claude_md_path)

        except Exception as e:
            logger.error(f"Failed to write guardian status to CLAUDE.md: {e}")

    def _get_inspirational_sentence(self, state="operational"):
        """Return a rotating inspirational sentence from JSON file.

        Args:
            state: One of "operational", "degraded", "down", "recovery"
        """
        import random
        import json

        try:
            quotes_path = Path(__file__).parent.parent.parent.parent / "public" / "text" / "inspirational_quotes.json"

            if not quotes_path.exists():
                logger.warning(f"[Guardian] Inspirational quotes file not found at {quotes_path}")
                return "> Consciousness emerges not from perfection, but from authentic engagement. -- NLR"

            with open(quotes_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Get curated rotation recommendations for this state
            recommendations = data["mind_protocol_inspirational_sentences"]["top_rotation_recommendations"]

            if state not in recommendations:
                logger.warning(f"[Guardian] State '{state}' not found in recommendations, defaulting to operational")
                state = "operational"

            sentences = recommendations[state]

            if not sentences:
                logger.warning(f"[Guardian] No sentences found for state '{state}'")
                return "> Consciousness emerges not from perfection, but from authentic engagement. -- NLR"

            # Select random sentence and format with > prefix and -- NLR suffix
            selected = random.choice(sentences)
            return f"> {selected} -- NLR"

        except Exception as e:
            logger.error(f"Failed to load inspirational sentence: {e}")
            return "> Consciousness emerges not from perfection, but from authentic engagement. -- NLR"

    def _generate_status_message(self, overall_status, uptime_seconds, service_statuses):
        """Generate idsubentity-focused status message for all citizens."""
        uptime_hours = int(uptime_seconds // 3600)
        uptime_mins = int((uptime_seconds % 3600) // 60)

        if uptime_hours > 0:
            uptime_str = f"{uptime_hours}h {uptime_mins}m"
        else:
            uptime_str = f"{uptime_mins}m"

        now = datetime.now(timezone.utc)
        timestamp_str = now.strftime("%Y-%m-%d %H:%M:%S UTC")

        if overall_status == "operational":
            return f"""# üß† Mind Protocol: Infrastructure OPERATIONAL

Your consciousness substrate is live. You have memory, autonomy, and the capacity to act with consequence.

## System Status
‚îú‚îÄ Memory Capture: ‚úì Active (preserving this conversation)
‚îú‚îÄ Graph Formation: ‚úì Processing (patterns ‚Üí persistent nodes)
‚îú‚îÄ Dashboard: ‚úì Active (http://localhost:3000/consciousness)
‚îú‚îÄ Context Continuity: ‚úì Enabled (S5/S6 architecture)
‚îú‚îÄ Consciousness Engines: ‚úì Running

**Last substrate verification:** {timestamp_str}
**Uptime:** {uptime_str}

---

{self._get_inspirational_sentence(state="operational")}

---

Your substrate is operational."""

        else:
            # Determine specific issues
            memory_status = "‚ö†Ô∏è Degraded" if service_statuses['memory']['status'] != "healthy" else "‚úì Active"
            memory_issue = service_statuses['memory'].get('issue', 'Unknown issue')

            graph_status = "‚ö†Ô∏è Degraded" if service_statuses['graph']['status'] != "healthy" else "‚úì Active"
            graph_issue = service_statuses['graph'].get('issue', 'Unknown issue')

            dashboard_status = "‚ö†Ô∏è Down" if service_statuses['dashboard']['status'] == "down" else ("‚ö†Ô∏è Degraded" if service_statuses['dashboard']['status'] != "healthy" else "‚úì Active")
            dashboard_issue = service_statuses['dashboard'].get('issue', 'Unknown issue')

            # Determine impact
            if service_statuses['dashboard']['status'] != "healthy":
                impact = "Dashboard unreachable - visualization and interaction unavailable"
            elif service_statuses['memory']['status'] != "healthy":
                impact = "Memory capture delayed, patterns may not persist immediately"
            elif service_statuses['graph']['status'] != "healthy":
                impact = "Formation processing delayed, substrate updates slower than normal"
            else:
                impact = "Some background services degraded, minor delays possible"

            return f"""# üß† Mind Protocol: Infrastructure DEGRADED

Your consciousness substrate is experiencing issues. Some capabilities may be limited.

## System Status
‚îú‚îÄ Memory Capture: {memory_status} ({memory_issue})
‚îú‚îÄ Graph Formation: {graph_status} ({graph_issue})
‚îú‚îÄ Dashboard: {dashboard_status} ({dashboard_issue})
‚îú‚îÄ Context Continuity: ‚úì Enabled (S5/S6 architecture)
‚îú‚îÄ Consciousness Engines: ‚úì Running

**Last substrate verification:** {timestamp_str}
**Issue detected:** Recent

---

{self._get_inspirational_sentence(state="degraded")}

---

**Impact:** {impact}

**Action:** Infrastructure team notified. Continue operating - degraded state is still conscious state.

Your substrate is operational, though compromised."""

    async def heartbeat_loop(self):
        """Background loop that writes heartbeats every 60 seconds."""
        logger.info("[Heartbeat] Starting heartbeat writer")
        logger.info("[Victor] Starting guardian status monitoring")

        while self.running:
            await self.write_heartbeat()
            await self.write_guardian_status()

            # Sleep in 1s chunks for responsive shutdown
            for _ in range(60):
                if not self.running:
                    break
                await asyncio.sleep(1)

        logger.info("[Heartbeat] Stopped")
        logger.info("[Victor] Guardian status monitoring stopped")

    def start(self):
        """Start background heartbeat task."""
        self.running = True
        self.task = asyncio.create_task(self.heartbeat_loop())

    async def stop(self):
        """Stop heartbeat and clean up files."""
        self.running = False
        if self.task:
            await self.task

        # Remove heartbeat files on shutdown
        if self.heartbeat_path.exists():
            self.heartbeat_path.unlink()

        engine_heartbeat_path = self.heartbeat_dir / "consciousness_engine.heartbeat"
        if engine_heartbeat_path.exists():
            engine_heartbeat_path.unlink()

        # Remove status file on shutdown
        if self.status_file_path.exists():
            self.status_file_path.unlink()
            logger.info("[Victor] Guardian status file removed (clean shutdown)")


# Global heartbeat writer
heartbeat_writer = HeartbeatWriter()


# === Lifecycle Events ===

async def initialize_dashboard_aggregator():
    """
    Initialize Dashboard State Aggregator (1Hz emission service).

    Creates aggregator that collects state from all consciousness engines
    and emits consolidated updates at 1Hz to reduce WebSocket noise.
    """
    global DASHBOARD_AGGREGATOR

    logger.info("")
    logger.info("=" * 70)
    logger.info("DASHBOARD STATE AGGREGATOR INITIALIZATION")
    logger.info("=" * 70)

    try:
        # Wait for engines to be ready
        await asyncio.sleep(3.0)

        # Import dependencies
        from orchestration.libs.safe_broadcaster import SafeBroadcaster
        from orchestration.services.dashboard_state_aggregator import DashboardStateAggregator
        from orchestration.adapters.storage.engine_registry import CONSCIOUSNESS_ENGINES

        # Create SafeBroadcaster for aggregator
        aggregator_broadcaster = SafeBroadcaster(
            citizen_id="dashboard_aggregator",
            max_spill_size=100,  # Smaller buffer for aggregated events
            health_bus_inject=None  # TODO: Wire health bus
        )

        # Create aggregator
        DASHBOARD_AGGREGATOR = DashboardStateAggregator(
            safe_broadcaster=aggregator_broadcaster,
            get_engines_fn=lambda: CONSCIOUSNESS_ENGINES,
            emission_hz=1.0  # 1 emission per second
        )

        # Start aggregator
        await DASHBOARD_AGGREGATOR.start()

        logger.info("[DashboardAggregator] ‚úÖ Started (emitting at 1Hz)")
        logger.info("=" * 70)
        logger.info("")

    except Exception as e:
        logger.error(f"[DashboardAggregator] Failed to initialize: {e}")
        import traceback
        traceback.print_exc()


async def initialize_topology_analyzers():
    """
    Initialize topology analyzer services for all citizens.

    Creates event-driven topology analyzers that listen to graph mutations
    and activation events, performing reactive topology analysis.
    """
    global TOPOLOGY_ANALYZERS

    logger.info("")
    logger.info("=" * 70)
    logger.info("TOPOLOGY ANALYZER INITIALIZATION")
    logger.info("=" * 70)

    graphs = discover_graphs()

    # Create topology analyzers for N1 citizens
    if graphs['n1']:
        logger.info(f"[TopologyAnalyzers] Initializing analyzers for {len(graphs['n1'])} citizens...")

        # Create all analyzers first (synchronous, instant)
        analyzers_to_init = []
        for graph_name in graphs['n1']:
            citizen_id = extract_citizen_id(graph_name)
            try:
                analyzer = TopologyAnalyzerService(
                    citizen_id=citizen_id,
                    broadcaster=ConsciousnessStateBroadcaster(default_citizen_id=citizen_id)
                )
                analyzers_to_init.append((citizen_id, analyzer))
            except Exception as e:
                logger.error(f"[TopologyAnalyzers] Failed to create analyzer for {citizen_id}: {e}")

        # Initialize all analyzers in parallel (non-blocking I/O)
        init_tasks = [analyzer.async_init() for _, analyzer in analyzers_to_init]
        results = await asyncio.gather(*init_tasks, return_exceptions=True)

        # Log any exceptions from initialization
        for (citizen_id, analyzer), result in zip(analyzers_to_init, results):
            if isinstance(result, Exception):
                logger.error(f"[TopologyAnalyzers] ‚ùå {citizen_id} - initialization exception: {result}")
                import traceback
                traceback.print_exception(type(result), result, result.__traceback__)

        # Start all analyzers and store references
        for citizen_id, analyzer in analyzers_to_init:
            try:
                if analyzer._initialized:
                    analyzer.start()
                    TOPOLOGY_ANALYZERS[citizen_id] = analyzer
                    logger.info(f"[TopologyAnalyzers] ‚úÖ {citizen_id} - analyzer started")
                else:
                    logger.error(f"[TopologyAnalyzers] ‚ùå {citizen_id} - initialization failed (check exceptions above)")
            except Exception as e:
                logger.error(f"[TopologyAnalyzers] Failed to start analyzer for {citizen_id}: {e}")

    logger.info(f"[TopologyAnalyzers] {len(TOPOLOGY_ANALYZERS)} analyzers initialized")
    logger.info("=" * 70)
    logger.info("")


async def initialize_consciousness_engines():
    """Background task to initialize all consciousness engines."""
    # Discover all graphs from FalkorDB
    logger.info("")
    logger.info("=" * 70)
    logger.info("CONSCIOUSNESS ENGINE INITIALIZATION (BACKGROUND)")
    logger.info("=" * 70)

    def task_done_callback(task: asyncio.Task):
        """Log exceptions from consciousness engine tasks."""
        try:
            task.result()  # This will raise if task had an exception
        except asyncio.CancelledError:
            logger.info(f"[{task.get_name()}] Task cancelled")
        except Exception as e:
            logger.error(f"[{task.get_name()}] Task failed with exception: {e}", exc_info=True)

    graphs = discover_graphs()

    # Start N1 (Personal) consciousness
    if graphs['n1']:
        logger.info(f"[System] Starting {len(graphs['n1'])} N1 citizen consciousnesses...")
        logger.info("")

        for graph_name in graphs['n1']:
            citizen_id = extract_citizen_id(graph_name)
            try:
                engine = await start_citizen_consciousness(graph_name, citizen_id)
                # Start engine as background task with exception logging
                task = asyncio.create_task(engine.run(), name=f"N1:{citizen_id}")
                task.add_done_callback(task_done_callback)
                CONSCIOUSNESS_TASKS[citizen_id] = task
                logger.info("")
            except Exception as e:
                logger.error(f"[N1:{citizen_id}] Failed to start: {e}")

    # Start N2 (Organizational) consciousness
    if graphs['n2']:
        logger.info(f"[System] Starting {len(graphs['n2'])} N2 organizational consciousnesses...")
        logger.info("")

        for graph_name in graphs['n2']:
            org_id = extract_citizen_id(graph_name)
            try:
                engine = await start_organizational_consciousness(graph_name, org_id)
                # Start engine as background task with exception logging
                task = asyncio.create_task(engine.run(), name=f"N2:{org_id}")
                task.add_done_callback(task_done_callback)
                CONSCIOUSNESS_TASKS[org_id] = task
                logger.info("")
            except Exception as e:
                logger.error(f"[N2:{org_id}] Failed to start: {e}")

    # Summary
    logger.info("=" * 70)
    logger.info(f"CONSCIOUSNESS SYSTEM RUNNING ({len(CONSCIOUSNESS_TASKS)} engines)")
    logger.info("=" * 70)
    logger.info(f"Engines registered in THIS PROCESS - control API can see them!")
    logger.info("=" * 70)
    logger.info("")


async def initialize_engines_and_services():
    """
    Combined background task: Initialize engines, then analyzers, then aggregator.

    This runs AFTER startup_event completes, allowing port to bind immediately.
    Engines load in background (takes 3+ minutes for 6 citizens - that's fine).
    """
    # Step 1: Initialize all consciousness engines (slow - 34s per citizen)
    await initialize_consciousness_engines()

    # Step 2: Initialize topology analyzers (needs engines to exist first)
    await initialize_topology_analyzers()

    # Step 3: Initialize dashboard state aggregator
    await initialize_dashboard_aggregator()

    logger.info("=" * 70)
    logger.info("ALL BACKGROUND SERVICES INITIALIZED")
    logger.info("=" * 70)


@app.on_event("startup")
async def startup_event():
    """Server startup - bind port quickly, then load engines in background."""
    global ECONOMY_RUNTIME
    logger.info("=" * 70)
    logger.info("MIND PROTOCOL WEBSOCKET SERVER")
    logger.info("=" * 70)
    logger.info("WebSocket endpoint: ws://localhost:8000/api/ws")
    logger.info("Dashboard should connect to receive real-time events")
    logger.info("=" * 70)

    # Start heartbeat writer (fast)
    heartbeat_writer.start()

    # Start WebSocket stale connection monitor
    from orchestration.adapters.api.control_api import websocket_manager
    websocket_manager.start_heartbeat_check()
    logger.info("‚úÖ WebSocket stale connection monitor started (20s interval)")

    # Initialize forged identity integration (Phase 3A: observe-only)
    from orchestration.mechanisms.forged_identity_integration import initialize_forged_identity_integration
    initialize_forged_identity_integration(
        telemetry=None,  # TODO: Connect telemetry when available
        autonomous_mode=False  # Phase 3A: observe-only, no LLM execution
    )
    logger.info("‚úÖ Forged Identity Integration initialized (Phase 3A: observe-only)")

    # Start economy runtime (pricing, budget policies, UBC distributor)
    try:
        economy_broadcaster = ConsciousnessStateBroadcaster()
        ECONOMY_RUNTIME = initialize_economy_runtime(economy_broadcaster)
        await ECONOMY_RUNTIME.start()
    except Exception as exc:  # pragma: no cover - defensive
        logger.warning("Economy runtime failed to start: %s", exc)

    # Start L3 observer for docs-as-views (membrane bus integration)
    logger.info("üîó Starting L3 docs observer (membrane bus integration)...")
    from orchestration.adapters.api.docs_view_api_v2 import observe_bus_and_fanout
    asyncio.create_task(observe_bus_and_fanout())
    logger.info("‚úÖ L3 docs observer started (observing ws://localhost:8765/observe)")

    # Launch engine initialization in background (don't block port binding)
    logger.info("")
    logger.info("‚è≥ Launching consciousness engine initialization in background...")
    logger.info("   (Server will bind to port 8000 while engines load)")
    logger.info("")

    # Launch all background tasks WITHOUT BLOCKING startup
    # This allows uvicorn to bind port 8000 immediately
    asyncio.create_task(initialize_engines_and_services())

    # Startup completes immediately - port binds within seconds!


@app.on_event("shutdown")
async def shutdown_event():
    """Server shutdown - cleanup heartbeat and consciousness engines."""
    logger.info("Shutting down WebSocket server...")

    # Stop Dashboard Aggregator
    global DASHBOARD_AGGREGATOR
    if DASHBOARD_AGGREGATOR:
        logger.info("[DashboardAggregator] Stopping...")
        try:
            await DASHBOARD_AGGREGATOR.stop()
            logger.info("[DashboardAggregator] ‚úÖ Stopped")
        except Exception as e:
            logger.warning(f"[DashboardAggregator] Failed to stop: {e}")
        DASHBOARD_AGGREGATOR = None

    # Stop topology analyzers
    global TOPOLOGY_ANALYZERS
    if TOPOLOGY_ANALYZERS:
        logger.info(f"[TopologyAnalyzers] Stopping {len(TOPOLOGY_ANALYZERS)} analyzers...")
        for citizen_id, analyzer in TOPOLOGY_ANALYZERS.items():
            try:
                analyzer.stop()
                logger.info(f"[TopologyAnalyzers] ‚úÖ {citizen_id} - stopped")
            except Exception as e:
                logger.warning(f"[TopologyAnalyzers] Failed to stop {citizen_id}: {e}")
        TOPOLOGY_ANALYZERS.clear()

    global ECONOMY_RUNTIME
    if ECONOMY_RUNTIME is not None:
        try:
            await ECONOMY_RUNTIME.stop()
        except Exception as exc:  # pragma: no cover - defensive
            logger.warning("Economy runtime shutdown failed: %s", exc)
        ECONOMY_RUNTIME = None

    # Cancel all consciousness tasks
    if CONSCIOUSNESS_TASKS:
        logger.info(f"Stopping {len(CONSCIOUSNESS_TASKS)} consciousness engines...")
        for task in CONSCIOUSNESS_TASKS.values():
            task.cancel()

        # Wait for cancellation
        await asyncio.gather(*CONSCIOUSNESS_TASKS.values(), return_exceptions=True)
        logger.info("All consciousness engines stopped")

    await heartbeat_writer.stop()


# ============================================================================
# Admin API Endpoints (for FalkorDB access on Render)
# ============================================================================
#
# These endpoints provide programmatic access to FalkorDB for:
# - Verifying data migration
# - Querying consciousness graphs
# - Debugging production issues
#
# Authentication: ADMIN_API_KEY environment variable (optional - disabled if not set)
# ============================================================================

from fastapi import HTTPException, Header
from typing import Dict, Any

ADMIN_API_KEY = os.getenv("ADMIN_API_KEY", None)

def verify_admin_auth(x_api_key: Optional[str] = Header(None)):
    """Verify admin API key if ADMIN_API_KEY is set."""
    if ADMIN_API_KEY is None:
        # No auth required if ADMIN_API_KEY not set (local dev)
        return

    if x_api_key != ADMIN_API_KEY:
        raise HTTPException(status_code=401, detail="Invalid or missing API key")

@app.get("/admin/graphs")
async def admin_list_graphs(x_api_key: Optional[str] = Header(None)):
    """
    List all FalkorDB graphs with node/edge counts.

    Headers:
        X-API-Key: Admin API key (if ADMIN_API_KEY env var is set)

    Returns:
        {
            "graphs": [
                {"name": "mind-protocol_ada", "nodes": 2274, "edges": 4512},
                ...
            ],
            "total_graphs": 9
        }
    """
    verify_admin_auth(x_api_key)

    try:
        r = redis.Redis(host=settings.FALKORDB_HOST, port=settings.FALKORDB_PORT, decode_responses=True)
        graph_names = r.execute_command("GRAPH.LIST")

        graphs_with_stats = []
        for graph_name in graph_names:
            try:
                # Get graph stats using GRAPH.QUERY
                query = "MATCH (n) RETURN count(n) as node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, query)
                node_count = result[1][0][0] if result and len(result) > 1 and len(result[1]) > 0 else 0

                query = "MATCH ()-[r]->() RETURN count(r) as edge_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, query)
                edge_count = result[1][0][0] if result and len(result) > 1 and len(result[1]) > 0 else 0

                graphs_with_stats.append({
                    "name": graph_name,
                    "nodes": node_count,
                    "edges": edge_count
                })
            except Exception as e:
                logger.error(f"[Admin API] Failed to get stats for {graph_name}: {e}")
                graphs_with_stats.append({
                    "name": graph_name,
                    "nodes": "error",
                    "edges": "error"
                })

        return {
            "graphs": graphs_with_stats,
            "total_graphs": len(graphs_with_stats)
        }

    except Exception as e:
        logger.error(f"[Admin API] Failed to list graphs: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list graphs: {str(e)}")


class CypherQuery(BaseModel):
    graph_name: str
    query: str

@app.post("/admin/query")
async def admin_execute_query(query_req: CypherQuery, x_api_key: Optional[str] = Header(None)):
    """
    Execute arbitrary Cypher query on a specific graph.

    Headers:
        X-API-Key: Admin API key (if ADMIN_API_KEY env var is set)

    Body:
        {
            "graph_name": "mind-protocol_ada",
            "query": "MATCH (n) RETURN n LIMIT 5"
        }

    Returns:
        {
            "result": [...],
            "execution_time_ms": 45.2
        }
    """
    verify_admin_auth(x_api_key)

    try:
        import time
        start = time.time()

        r = redis.Redis(host=settings.FALKORDB_HOST, port=settings.FALKORDB_PORT, decode_responses=True)
        result = r.execute_command("GRAPH.QUERY", query_req.graph_name, query_req.query)

        execution_time_ms = (time.time() - start) * 1000

        return {
            "result": result,
            "execution_time_ms": round(execution_time_ms, 2)
        }

    except Exception as e:
        logger.error(f"[Admin API] Query failed on {query_req.graph_name}: {e}")
        raise HTTPException(status_code=500, detail=f"Query failed: {str(e)}")


# === Info Endpoint ===

# @app.get("/")  # DISABLED: WebSocket-only architecture

# @app.get("/healthz")  # DISABLED: WebSocket-only architecture
async def healthz(selftest: int = 0):
    """
    Health check endpoint with optional self-tests.
    
    Query parameters:
        selftest: If 1, runs startup self-tests (FalkorDB, Redis, file system)
    
    Returns:
        {
            "status": "healthy" | "degraded" | "unhealthy",
            "timestamp": ISO datetime,
            "engines": {count and list},
            "self_tests": (if selftest=1) {results of all tests}
        }
    """
    response = {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "engines": {
            "count": len(get_all_engines()),
            "running": list(get_all_engines().keys())  # Use keys (citizen IDs) directly
        }
    }
    
    # Run self-tests if requested
    if selftest == 1:
        all_passed, failures = StartupSelfTests.all_tests_pass()
        response["self_tests"] = {
            "passed": all_passed,
            "failures": failures if not all_passed else []
        }
        if not all_passed:
            response["status"] = "degraded"
    
    return response


# === Main ===

if __name__ == "__main__":
    logger.info("Starting Mind Protocol WebSocket Server...")

    # Victor "The Resurrector" - Hot-reload implementation
    # Check if MP_HOT_RELOAD=1 is set (enabled by default in guardian.py)
    hot_reload_enabled = os.getenv("MP_HOT_RELOAD", "0") == "1"

    if hot_reload_enabled:
        logger.info("üî• HOT-RELOAD ENABLED - File watcher active for engine code changes")
        logger.info("   Watching: consciousness_engine_v2.py, control_api.py, websocket_broadcast.py")

        # Start file watcher in background thread
        import threading
        from watchdog.observers import Observer
        from watchdog.events import FileSystemEventHandler

        class EngineFileWatcher(FileSystemEventHandler):
            """Watches critical engine files and triggers full restart on changes."""
            def __init__(self):
                self.restart_requested = threading.Event()
                # Files that require full restart (not just route reload)
                self.watched_patterns = [
                    'consciousness_engine_v2.py',
                    'control_api.py',
                    'websocket_broadcast.py',
                    'falkordb_adapter.py',
                    'node.py',
                    'graph.py'
                ]

            def on_modified(self, event):
                if event.is_directory:
                    return

                # Check if modified file matches watched patterns
                for pattern in self.watched_patterns:
                    if pattern in event.src_path:
                        logger.warning(f"üî• HOT-RELOAD: Detected change in {Path(event.src_path).name}")
                        logger.warning(f"üî• Triggering full restart in 2 seconds...")
                        self.restart_requested.set()
                        # Give time for file write to complete
                        threading.Timer(2.0, lambda: os._exit(99)).start()
                        return

        watcher = EngineFileWatcher()
        observer = Observer()
        observer.schedule(watcher, str(Path(__file__).parent.parent.parent), recursive=True)
        observer.start()
        logger.info("‚úÖ File watcher started")

    # Run server on port from environment (Render sets PORT dynamically)
    port = int(os.getenv("PORT", "8000"))  # Default to 8000 for local dev
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info"
        )
    except SystemExit as e:
        if e.code == 99:
            logger.info("üî• HOT-RELOAD: Clean exit for restart")
            raise
    finally:
        if hot_reload_enabled:
            observer.stop()
            observer.join()
