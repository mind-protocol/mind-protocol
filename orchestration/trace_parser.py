"""
TRACE Format Parser - Extract consciousness learning signals from autonomous thinking

Parses THE_TRACE_FORMAT.md consciousness stream format to extract:
1. Reinforcement signals: [node_id: very useful] → weight updates
2. Node formations: [NODE_FORMATION: ...] → new node creation
3. Link formations: [LINK_FORMATION: ...] → new link creation

This parser enables dual learning modes:
- Reinforcement: Existing nodes strengthened/weakened based on usefulness
- Formation: New consciousness chunks created with rich metadata

Author: Felix "Ironhand"
Date: 2025-10-19
Pattern: Autonomous consciousness substrate learning loop
"""

import re
import sys
import os
import json
import redis
from typing import List, Dict, Any, Tuple, Set
from datetime import datetime
from pathlib import Path
import logging

# Add substrate to path for schema import
sys.path.insert(0, str(Path(__file__).parent.parent))
from substrate.schemas.consciousness_schema import NODE_TYPES, RELATION_TYPES

# Import embedding service for consciousness substrate embeddings
from orchestration.embedding_service import get_embedding_service

logger = logging.getLogger(__name__)

# Valid type names for validation
VALID_NODE_TYPES = {node_class.__name__ for node_class in NODE_TYPES}
VALID_LINK_TYPES = {relation_class.__name__ for relation_class in RELATION_TYPES}

# Schema registry connection (cached)
_schema_cache = None


def _load_schema_registry():
    """Load schema registry from FalkorDB (cached)."""
    global _schema_cache
    if _schema_cache is not None:
        return _schema_cache

    try:
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)

        # Get universal required node fields (inherited by ALL node types)
        # Exclude bitemporal_tracking fields (auto-generated by Pydantic defaults)
        # Exclude node_type (extracted from formation header, not fields block)
        result = r.execute_command('GRAPH.QUERY', 'schema_registry',
            '''MATCH (f:UniversalNodeField {required: true})
               WHERE f.category <> 'bitemporal_tracking' AND f.name <> 'node_type'
               RETURN f.name'''
        )
        universal_node_fields = {row[0] for row in result[1]}

        # Get universal required link fields (inherited by ALL link types)
        # Exclude bitemporal_tracking fields (auto-generated by Pydantic defaults)
        result = r.execute_command('GRAPH.QUERY', 'schema_registry',
            '''MATCH (f:UniversalLinkField {required: true})
               WHERE f.category <> 'bitemporal_tracking'
               RETURN f.name'''
        )
        universal_link_fields = {row[0] for row in result[1]}

        # Get type-specific required fields for all node types
        result = r.execute_command('GRAPH.QUERY', 'schema_registry',
            '''MATCH (nt:NodeTypeSchema)-[:HAS_REQUIRED_FIELD]->(f:FieldSchema)
               RETURN nt.type_name, f.name'''
        )

        node_required_fields = {}
        for row in result[1]:
            type_name = row[0]
            field_name = row[1]
            if type_name not in node_required_fields:
                # Start with universal fields for this type
                node_required_fields[type_name] = set(universal_node_fields)
            # Add type-specific field
            node_required_fields[type_name].add(field_name)

        # Get type-specific required fields for all link types
        result = r.execute_command('GRAPH.QUERY', 'schema_registry',
            '''MATCH (lt:LinkTypeSchema)-[:HAS_REQUIRED_FIELD]->(f:FieldSchema)
               RETURN lt.type_name, f.name'''
        )

        link_required_fields = {}
        for row in result[1]:
            type_name = row[0]
            field_name = row[1]
            if type_name not in link_required_fields:
                # Start with universal fields for this type
                link_required_fields[type_name] = set(universal_link_fields)
            # Add type-specific field
            link_required_fields[type_name].add(field_name)

        _schema_cache = {
            'node_required_fields': node_required_fields,
            'link_required_fields': link_required_fields,
            'universal_node_fields': universal_node_fields,
            'universal_link_fields': universal_link_fields
        }

        logger.info(f"[TraceParser] Loaded schema registry: {len(node_required_fields)} node types, {len(link_required_fields)} link types")
        logger.info(f"[TraceParser] Universal fields: {len(universal_node_fields)} node, {len(universal_link_fields)} link")
        return _schema_cache

    except Exception as e:
        logger.warning(f"[TraceParser] Failed to load schema registry: {e}. Using fallback validation.")
        return None


class TraceParseResult:
    """Result of parsing TRACE format consciousness stream."""

    def __init__(self):
        # Reinforcement signals
        self.reinforcement_signals: List[Dict[str, Any]] = []  # {node_id, usefulness_level, adjustment}

        # NEW (Phase 2): Hamilton apportionment seats
        self.reinforcement_seats: Dict[str, int] = {}  # {node_id: integer_seats_allocated}

        # Formation blocks
        self.node_formations: List[Dict[str, Any]] = []  # {node_type, fields}
        self.link_formations: List[Dict[str, Any]] = []  # {link_type, source, target, fields}

        # Entity activation tracking
        self.entity_activations: Dict[str, str] = {}  # {entity_id: activation_level}

        # Metadata
        self.energy_level: str = None
        self.primary_entity: str = None


class TraceParser:
    """Parser for TRACE format consciousness streams."""

    # Usefulness level → reinforcement_weight adjustment
    USEFULNESS_ADJUSTMENTS = {
        'very useful': +0.15,
        'useful': +0.10,
        'somewhat useful': +0.05,
        'not useful': -0.05,
        'misleading': -0.15
    }

    def __init__(self, enable_embeddings: bool = True):
        """
        Initialize parser with regex patterns.

        Args:
            enable_embeddings: If True, generate embeddings during formation parsing
                              (requires sentence-transformers or Ollama)
        """

        # Inline reinforcement pattern: [node_id: very useful]
        self.reinforcement_pattern = re.compile(
            r'\[(?P<node_id>[a-zA-Z0-9_]+):\s*(?P<usefulness>very useful|useful|somewhat useful|not useful|misleading)\]'
        )

        # Node formation block pattern
        self.node_formation_pattern = re.compile(
            r'\[NODE_FORMATION:\s*(?P<node_type>[a-zA-Z_]+)\]\s*\n(?P<fields>(?:^[a-z_]+:.*$\n?)+)',
            re.MULTILINE
        )

        # Link formation block pattern
        self.link_formation_pattern = re.compile(
            r'\[LINK_FORMATION:\s*(?P<link_type>[A-Z_]+)\]\s*\n(?P<fields>(?:^[a-z_]+:.*$\n?)+)',
            re.MULTILINE
        )

        # Entity activation pattern: [entity_name: dominant|strong|moderate|weak]
        self.entity_activation_pattern = re.compile(
            r'\[(?P<entity>[a-z_]+):\s*(?P<level>dominant|strong|moderate|weak|absent)\]'
        )

        # Energy level pattern: **Energy Level:** [Peak - ...]
        self.energy_pattern = re.compile(
            r'\*\*Energy Level:\*\*\s*\[(?P<level>Dormant|Resting|Alert|Focused|Engaged|Energized|Peak|Overwhelmed)[^\]]*\]'
        )

        # Embedding service (lazy-loaded)
        self.enable_embeddings = enable_embeddings
        self._embedding_service = None

    def _get_embedding_service(self):
        """Get or initialize embedding service (lazy-loaded)."""
        if not self.enable_embeddings:
            return None

        if self._embedding_service is None:
            try:
                self._embedding_service = get_embedding_service()
                logger.info("[TraceParser] Embedding service initialized")
            except Exception as e:
                logger.warning(f"[TraceParser] Failed to initialize embedding service: {e}")
                logger.warning("[TraceParser] Formations will be created WITHOUT embeddings")
                self.enable_embeddings = False

        return self._embedding_service

    def parse(self, content: str) -> TraceParseResult:
        """
        Parse TRACE format consciousness stream.

        Args:
            content: Raw consciousness stream text

        Returns:
            TraceParseResult with extracted learning signals
        """
        result = TraceParseResult()

        # Extract reinforcement signals
        result.reinforcement_signals = self._extract_reinforcement_signals(content)

        # NEW (Phase 2): Apply Hamilton apportionment to reinforcement signals
        result.reinforcement_seats = self._apply_hamilton_apportionment(result.reinforcement_signals)

        # Extract node formations
        result.node_formations = self._extract_node_formations(content)

        # Extract link formations
        result.link_formations = self._extract_link_formations(content)

        # Extract entity activations
        result.entity_activations = self._extract_entity_activations(content)

        # Extract energy level
        result.energy_level = self._extract_energy_level(content)

        # Determine primary entity
        result.primary_entity = self._determine_primary_entity(result.entity_activations)

        return result

    def _extract_reinforcement_signals(self, content: str) -> List[Dict[str, Any]]:
        """Extract inline reinforcement signals: [node_id: very useful]"""
        signals = []

        for match in self.reinforcement_pattern.finditer(content):
            node_id = match.group('node_id')
            usefulness = match.group('usefulness')
            adjustment = self.USEFULNESS_ADJUSTMENTS.get(usefulness, 0)

            signals.append({
                'node_id': node_id,
                'usefulness_level': usefulness,
                'adjustment': adjustment
            })

        logger.debug(f"[TraceParser] Found {len(signals)} reinforcement signals")
        return signals

    def _extract_node_formations(self, content: str) -> List[Dict[str, Any]]:
        """Extract node formation blocks: [NODE_FORMATION: NodeType]"""
        formations = []

        for match in self.node_formation_pattern.finditer(content):
            node_type = match.group('node_type')
            fields_block = match.group('fields')

            # Validate node type exists in schema
            if node_type not in VALID_NODE_TYPES:
                logger.warning(
                    f"[TraceParser] Rejecting node formation with invalid type '{node_type}'. "
                    f"Valid types: {sorted(VALID_NODE_TYPES)}"
                )
                continue

            # Parse field: value pairs
            fields = self._parse_field_block(fields_block)

            # Validate required fields from schema registry
            if not self._validate_node_fields(fields, node_type):
                logger.warning(f"[TraceParser] Skipping invalid node formation: {node_type}")
                continue

            # Generate embeddings if enabled
            if self.enable_embeddings:
                embedding_service = self._get_embedding_service()
                if embedding_service:
                    try:
                        embeddable_text, embedding = embedding_service.create_formation_embedding(
                            'node', node_type, fields
                        )
                        fields['embeddable_text'] = embeddable_text
                        fields['content_embedding'] = embedding
                        logger.debug(f"[TraceParser] Generated embedding for {node_type} node (text len: {len(embeddable_text)})")
                    except Exception as e:
                        logger.warning(f"[TraceParser] Failed to generate embedding for {node_type}: {e}")

            # NEW (Phase 2): Calculate formation quality
            scope = fields.get('scope', 'personal')
            quality_metrics = self._calculate_formation_quality(fields, node_type, scope)

            formations.append({
                'node_type': node_type,
                'fields': fields,
                # Phase 2: Quality metrics for weight learning
                'quality': quality_metrics['quality'],
                'completeness': quality_metrics['completeness'],
                'evidence': quality_metrics['evidence'],
                'novelty': quality_metrics['novelty']
            })

        logger.info(f"[TraceParser] Found {len(formations)} node formations")
        return formations

    def _extract_link_formations(self, content: str) -> List[Dict[str, Any]]:
        """Extract link formation blocks: [LINK_FORMATION: LINK_TYPE]"""
        formations = []

        for match in self.link_formation_pattern.finditer(content):
            link_type = match.group('link_type')
            fields_block = match.group('fields')

            # Validate link type exists in schema
            if link_type not in VALID_LINK_TYPES:
                logger.warning(
                    f"[TraceParser] Rejecting link formation with invalid type '{link_type}'. "
                    f"Valid types: {sorted(VALID_LINK_TYPES)}"
                )
                continue

            # Parse field: value pairs
            fields = self._parse_field_block(fields_block)

            # Validate required fields from schema registry
            if not self._validate_link_fields(fields, link_type):
                logger.warning(f"[TraceParser] Skipping invalid link formation: {link_type}")
                continue

            # Extract source/target before embedding generation (they're not semantic content)
            source = fields.pop('source')
            target = fields.pop('target')

            # Generate embeddings if enabled
            if self.enable_embeddings:
                embedding_service = self._get_embedding_service()
                if embedding_service:
                    try:
                        embeddable_text, embedding = embedding_service.create_formation_embedding(
                            'link', link_type, fields
                        )
                        fields['embeddable_text'] = embeddable_text
                        fields['relationship_embedding'] = embedding
                        logger.debug(f"[TraceParser] Generated embedding for {link_type} link (text len: {len(embeddable_text)})")
                    except Exception as e:
                        logger.warning(f"[TraceParser] Failed to generate embedding for {link_type}: {e}")

            formations.append({
                'link_type': link_type,
                'source': source,
                'target': target,
                'fields': fields
            })

        logger.info(f"[TraceParser] Found {len(formations)} link formations")
        return formations

    def _extract_entity_activations(self, content: str) -> Dict[str, str]:
        """Extract entity activation levels: [entity_name: strong]"""
        activations = {}

        for match in self.entity_activation_pattern.finditer(content):
            entity = match.group('entity')
            level = match.group('level')

            # Keep highest activation level for each entity
            if entity not in activations or self._is_higher_activation(level, activations[entity]):
                activations[entity] = level

        logger.debug(f"[TraceParser] Found {len(activations)} entity activations")
        return activations

    def _extract_energy_level(self, content: str) -> str:
        """Extract energy level from consciousness stream."""
        match = self.energy_pattern.search(content)
        if match:
            return match.group('level')
        return None

    def _parse_field_block(self, fields_block: str) -> Dict[str, Any]:
        """
        Parse field: value pairs from formation block.

        Supports multi-line array/object values:
            name: "node_identifier"
            how_to_apply: [
              "step 1",
              "step 2"
            ]
            confidence: 0.95
        """
        fields = {}
        lines = fields_block.strip().split('\n')
        i = 0

        while i < len(lines):
            line = lines[i]

            # Skip lines without field names (continuation lines handled below)
            if ':' not in line:
                i += 1
                continue

            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip()

            # Check for multi-line JSON arrays/objects
            if value.startswith('[') and not value.endswith(']'):
                # Accumulate until closing bracket
                while i < len(lines) - 1 and not value.rstrip().endswith(']'):
                    i += 1
                    value += '\n' + lines[i]
            elif value.startswith('{') and not value.endswith('}'):
                # Accumulate until closing brace
                while i < len(lines) - 1 and not value.rstrip().endswith('}'):
                    i += 1
                    value += '\n' + lines[i]

            value = value.strip()

            # Parse JSON arrays and objects
            if (value.startswith('[') and value.endswith(']')) or \
               (value.startswith('{') and value.endswith('}')):
                try:
                    value = json.loads(value)
                    fields[key] = value
                    logger.debug(f"[TraceParser] Parsed JSON field '{key}': {value}")
                    i += 1
                    continue
                except json.JSONDecodeError as e:
                    logger.warning(f"[TraceParser] Failed to parse JSON for '{key}': {e}\nValue: {value[:200]}")
                    # Fall through to string parsing

            # Remove quotes from strings
            if value.startswith('"') and value.endswith('"'):
                value = value[1:-1]

            # Parse numeric values
            try:
                if '.' in value:
                    value = float(value)
                elif value.isdigit():
                    value = int(value)
            except ValueError:
                pass

            fields[key] = value
            i += 1

        logger.debug(f"[TraceParser] Parsed fields: {list(fields.keys())}")
        return fields

    def _validate_node_fields(self, fields: Dict[str, Any], node_type: str) -> bool:
        """
        Validate node has required fields from schema registry.

        Args:
            fields: Parsed fields from formation block
            node_type: Type of node being created

        Returns:
            True if all required fields present, False otherwise
        """
        # Load schema registry (cached)
        schema = _load_schema_registry()

        if schema and node_type in schema['node_required_fields']:
            # Use schema registry as source of truth
            required_fields = schema['node_required_fields'][node_type]

            # Add 'scope' - required for routing but not in schema registry
            # (scope is infrastructure, not schema)
            required_fields_with_scope = required_fields | {'scope'}

            missing = [f for f in required_fields_with_scope if f not in fields]

            if missing:
                logger.warning(f"[TraceParser] Missing required fields for {node_type}: {missing}")
                return False

        else:
            # Fallback validation if schema registry unavailable
            logger.warning(f"[TraceParser] Schema registry not available, using fallback validation for {node_type}")

            # Universal required fields
            universal_required = ['name', 'scope', 'confidence', 'formation_trigger']
            if not all(field in fields for field in universal_required):
                missing = [f for f in universal_required if f not in fields]
                logger.warning(f"[TraceParser] Missing universal required fields: {missing}")
                return False

        # Validate scope value
        valid_scopes = ['personal', 'organizational', 'ecosystem']
        if fields.get('scope') not in valid_scopes:
            logger.warning(f"[TraceParser] Invalid scope '{fields.get('scope')}'. Must be one of: {valid_scopes}")
            return False

        return True

    def _validate_link_fields(self, fields: Dict[str, Any], link_type: str) -> bool:
        """
        Validate link has required fields from schema registry.

        Args:
            fields: Parsed fields from formation block
            link_type: Type of link being created

        Returns:
            True if all required fields present, False otherwise
        """
        # Load schema registry (cached)
        schema = _load_schema_registry()

        if schema and link_type in schema['link_required_fields']:
            # Use schema registry as source of truth
            required_fields = schema['link_required_fields'][link_type]

            # Add 'scope' - required for routing but not in schema registry
            # (scope is infrastructure, not schema)
            required_fields_with_scope = required_fields | {'scope'}

            missing = [f for f in required_fields_with_scope if f not in fields]

            if missing:
                logger.warning(f"[TraceParser] Missing required fields for {link_type}: {missing}")
                return False

        else:
            # Fallback validation if schema registry unavailable
            logger.warning(f"[TraceParser] Schema registry not available, using fallback validation for {link_type}")

            # Universal required fields
            universal_required = ['source', 'target', 'scope', 'goal', 'mindstate', 'confidence', 'formation_trigger']
            if not all(field in fields for field in universal_required):
                missing = [f for f in universal_required if f not in fields]
                logger.warning(f"[TraceParser] Missing universal required fields: {missing}")
                return False

        # Validate scope value
        valid_scopes = ['personal', 'organizational', 'ecosystem']
        if fields.get('scope') not in valid_scopes:
            logger.warning(f"[TraceParser] Invalid scope '{fields.get('scope')}'. Must be one of: {valid_scopes}")
            return False

        return True

    def _is_higher_activation(self, level1: str, level2: str) -> bool:
        """Compare activation levels."""
        hierarchy = ['absent', 'weak', 'moderate', 'strong', 'dominant']
        try:
            return hierarchy.index(level1) > hierarchy.index(level2)
        except ValueError:
            return False

    def _determine_primary_entity(self, activations: Dict[str, str]) -> str:
        """Determine primary (most activated) entity."""
        if not activations:
            return None

        # Sort by activation level
        hierarchy = ['absent', 'weak', 'moderate', 'strong', 'dominant']

        sorted_entities = sorted(
            activations.items(),
            key=lambda x: hierarchy.index(x[1]) if x[1] in hierarchy else -1,
            reverse=True
        )

        if sorted_entities:
            return sorted_entities[0][0]

        return None

    # =========================================================================
    # Phase 2: Learning Infrastructure Enhancements
    # =========================================================================

    def _apply_hamilton_apportionment(self, signals: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Apply Hamilton apportionment to convert usefulness marks to integer seats.

        Hamilton's method ensures fair representation:
        - Total seats = constant (100)
        - Each usefulness level gets quota based on weight
        - Remaining seats distributed to highest remainders

        Args:
            signals: List of reinforcement signals with usefulness levels

        Returns:
            Dict mapping node_id to allocated seats: {node_id: seats}

        Reference: https://en.wikipedia.org/wiki/Largest_remainder_method
        """
        if not signals:
            return {}

        # Total seats to allocate
        TOTAL_SEATS = 100

        # Usefulness weight (same as adjustment, but absolute value for allocation)
        usefulness_weights = {
            'very useful': 0.15,
            'useful': 0.10,
            'somewhat useful': 0.05,
            'not useful': 0.05,    # Same magnitude as somewhat useful
            'misleading': 0.15      # Same magnitude as very useful
        }

        # Calculate total weight
        total_weight = sum(
            usefulness_weights.get(sig['usefulness_level'], 0)
            for sig in signals
        )

        if total_weight == 0:
            logger.warning("[TraceParser] Total weight is zero, no seats to allocate")
            return {}

        # Calculate quota (proportional allocation) for each signal
        quotas = []
        for sig in signals:
            weight = usefulness_weights.get(sig['usefulness_level'], 0)
            quota = (weight / total_weight) * TOTAL_SEATS
            quotas.append({
                'node_id': sig['node_id'],
                'usefulness_level': sig['usefulness_level'],
                'quota': quota,
                'integer_part': int(quota),
                'remainder': quota - int(quota)
            })

        # Allocate integer parts first
        seats_allocated = {q['node_id']: q['integer_part'] for q in quotas}
        total_allocated = sum(seats_allocated.values())

        # Distribute remaining seats to highest remainders (Hamilton's method)
        remaining_seats = TOTAL_SEATS - total_allocated
        if remaining_seats > 0:
            # Sort by remainder descending
            quotas_by_remainder = sorted(quotas, key=lambda x: x['remainder'], reverse=True)

            for i in range(remaining_seats):
                node_id = quotas_by_remainder[i]['node_id']
                seats_allocated[node_id] += 1

        logger.debug(f"[TraceParser] Hamilton apportionment: {len(signals)} signals → {sum(seats_allocated.values())} seats")
        logger.debug(f"[TraceParser] Seat distribution: {seats_allocated}")

        return seats_allocated

    def _calculate_formation_quality(
        self,
        fields: Dict[str, Any],
        node_type: str,
        scope: str
    ) -> Dict[str, float]:
        """
        Calculate formation quality: (Completeness × Evidence × Novelty)^(1/3)

        Args:
            fields: Formation fields
            node_type: Type of node being formed
            scope: Graph scope (personal/organizational/ecosystem)

        Returns:
            Dict with quality components: {quality, completeness, evidence, novelty}
        """
        # Calculate each component
        completeness = self._calculate_completeness(fields, node_type)
        evidence = self._calculate_evidence(fields, scope)
        novelty = self._calculate_novelty(fields, node_type, scope)

        # Geometric mean: (C × E × N)^(1/3)
        quality = (completeness * evidence * novelty) ** (1/3)

        logger.debug(
            f"[TraceParser] Formation quality for {node_type}: "
            f"C={completeness:.2f}, E={evidence:.2f}, N={novelty:.2f} → Q={quality:.2f}"
        )

        return {
            'quality': quality,
            'completeness': completeness,
            'evidence': evidence,
            'novelty': novelty
        }

    def _calculate_completeness(self, fields: Dict[str, Any], node_type: str) -> float:
        """
        Calculate completeness: fraction of fields filled with substantial content.

        C = (fields_with_content / total_expected_fields)

        Args:
            fields: Formation fields
            node_type: Type of node

        Returns:
            Completeness score [0, 1]
        """
        # Load schema to get expected fields
        schema = _load_schema_registry()

        if schema and node_type in schema['node_required_fields']:
            required_fields = schema['node_required_fields'][node_type]
            total_fields = len(required_fields)
        else:
            # Fallback: count all fields present
            total_fields = max(len(fields), 5)  # Assume at least 5 fields

        # Count fields with substantial content
        substantial_count = 0
        for key, value in fields.items():
            # Skip infrastructure fields
            if key in ['name', 'scope', 'node_type', 'created_by', 'substrate']:
                continue

            # Check if value is substantial
            if isinstance(value, str) and len(value.strip()) > 10:
                substantial_count += 1
            elif isinstance(value, (int, float)) and value != 0:
                substantial_count += 1
            elif isinstance(value, list) and len(value) > 0:
                substantial_count += 1
            elif isinstance(value, dict) and len(value) > 0:
                substantial_count += 1

        completeness = min(substantial_count / max(total_fields, 1), 1.0)

        logger.debug(f"[TraceParser] Completeness: {substantial_count}/{total_fields} = {completeness:.2f}")

        return completeness

    def _calculate_evidence(self, fields: Dict[str, Any], scope: str) -> float:
        """
        Calculate evidence: connection quality to existing high-weight nodes.

        E = average weight of referenced nodes (if any exist)

        For now, return default 0.5 (Phase 3 will add graph querying)

        Args:
            fields: Formation fields
            scope: Graph scope

        Returns:
            Evidence score [0, 1]
        """
        # TODO (Phase 3): Query graph for referenced node weights
        # For now, return neutral evidence score
        #
        # Future implementation will:
        # 1. Extract node references from fields
        # 2. Query graph for those nodes' log_weight
        # 3. Return average weight of found nodes

        evidence = 0.5  # Neutral default

        logger.debug(f"[TraceParser] Evidence: {evidence:.2f} (default - Phase 3 will query graph)")

        return evidence

    def _calculate_novelty(self, fields: Dict[str, Any], node_type: str, scope: str) -> float:
        """
        Calculate novelty: is this genuinely new content?

        N = 1.0 - (similarity to existing nodes of same type)

        For now, return default 0.7 (Phase 3 will add embedding similarity)

        Args:
            fields: Formation fields
            node_type: Type of node
            scope: Graph scope

        Returns:
            Novelty score [0, 1]
        """
        # TODO (Phase 3): Calculate semantic similarity to existing nodes
        # For now, return optimistic novelty score
        #
        # Future implementation will:
        # 1. Generate embedding for this formation
        # 2. Query similar nodes of same type in graph
        # 3. Calculate 1 - max_similarity

        novelty = 0.7  # Optimistic default (assume somewhat novel)

        logger.debug(f"[TraceParser] Novelty: {novelty:.2f} (default - Phase 3 will use embeddings)")

        return novelty


def parse_trace_format(content: str) -> TraceParseResult:
    """
    Parse TRACE format consciousness stream.

    Args:
        content: Raw consciousness stream text using THE_TRACE_FORMAT

    Returns:
        TraceParseResult with extracted learning signals
    """
    parser = TraceParser()
    return parser.parse(content)


if __name__ == "__main__":
    # Test with sample TRACE format
    sample = """
Nicolas asks about the schema registry [node_schema_completion: very useful]
and wants the type reference automated [node_automation_principle: useful].

**Activated Mode:**
The Builder [builder: dominant] wants to implement immediately.
The Validator [validator: moderate] wants to verify understanding.

**Triggered Node:**
The discussion about single source of truth [node_single_source_truth: very useful]
reminds me of the schema fragmentation problem [node_schema_fragmentation: somewhat useful].

**Emergent Story:**
I realize the schema registry needs to be queryable AND auto-generate docs.

[NODE_FORMATION: Principle]
name: "queryable_schema_as_source_of_truth"
principle_statement: "Schema registry in FalkorDB should be the authoritative source that auto-generates all documentation"
why_it_matters: "Eliminates 'is this up to date?' questions by making docs always current"
confidence: 0.9
formation_trigger: "systematic_analysis"

[LINK_FORMATION: ENABLES]
source: "queryable_schema_as_source_of_truth"
target: "automated_type_reference_generation"
goal: "Having queryable schema enables automatic doc generation without manual sync"
mindstate: "Clarity emerging about architecture pattern"
energy: 0.85
confidence: 0.9
formation_trigger: "spontaneous_insight"
enabling_type: "prerequisite"
degree_of_necessity: "required"
felt_as: "Pieces clicking together - the architecture reveals itself"
without_this: "Would need manual doc updates, leading to staleness and distrust"

**Energy Level:** [Peak - complete architectural clarity]
"""

    result = parse_trace_format(sample)

    print(f"Reinforcement Signals: {len(result.reinforcement_signals)}")
    for sig in result.reinforcement_signals:
        print(f"  - {sig['node_id']}: {sig['usefulness_level']} ({sig['adjustment']:+.2f})")

    print(f"\nNode Formations: {len(result.node_formations)}")
    for node in result.node_formations:
        print(f"  - {node['node_type']}: {node['fields'].get('name')}")

    print(f"\nLink Formations: {len(result.link_formations)}")
    for link in result.link_formations:
        print(f"  - {link['link_type']}: {link['source']} -> {link['target']}")

    print(f"\nEntity Activations:")
    for entity, level in result.entity_activations.items():
        print(f"  - {entity}: {level}")

    print(f"\nPrimary Entity: {result.primary_entity}")
    print(f"Energy Level: {result.energy_level}")
